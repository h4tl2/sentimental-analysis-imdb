{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis using CNN, LSTM, CNN+LSTM  \n",
    "Nivit Nantanivattikul - 5833638023  \n",
    "Tatchanon Kummalue - 5833630023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### แรงจูงใจ (Motivation)\n",
    "<p>&emsp;&emsp;เราพูดได้ว่าทุกวันนี้โลกธุรกิจของเรากำลังถูกขับเคลื่อนด้วยข้อมูลเป็นส่วนมาก หรือที่เรียกว่า Data driven business ถ้าบริษัทไหนมี knowledge มากกว่าหรือมีข้อมูลในมือมากกว่าก็จะได้เปรียบในการพัฒนาบริษัทได้มากกว่า แต่หากว่าบริษัทมีแต่ข้อมูลแต่ไม่สามารถดึงเอาสิ่งที่มีประโยชน์ออกมาจากข้อมูลได้ ข้อมูลนั้นก็จะไร้ซึ่งประโยชน์ เราจึงเห็นได้ว่าทุกวันนี้มีรูปแบบการนำเอาโมเดลต่างๆ ไปเรียนรู้ข้อมูลเพื่อที่จะทำให้เราได้ Insights ออกมาจากข้อมูลได้ วิธีการอย่างนึงที่นำมาใช้วัดผลลัพธ์ของ Campaign marketing ต่างๆก็คือการวัดผลตอบรับจากทางกระแส Social media ต่างๆ แต่การที่จะให้มนุษย์มานั่ง Label data จะส่งผลให้เสียทรัพยากรไปอย่างเปล่าประโยชน์ ดังนั้นเราจึงจะเห็นได้ว่าการทำ Sentimental analysis กำลังเป็นกระแสในช่วงนี้<p>\n",
    "<p>&emsp;&emsp;เราจึงต้องการที่จะนำเอาโมเดลที่ได้พัฒนาจาก Dataset นี้ ไปชี้วัด Feedback จาก Marketing campaign ต่างๆได้ โดยดูผลตอบรับทาง social media ต่างๆที่มี feed ข้อมูลจำนวนมากไหลเวียนอยู่ตลอดเวลา ทำให้บริษัทสามารถวัดได้ว่า Campaign ต่างๆที่ Launch ไปนั้น ROI เหมาะสมหรือไม่ และควรที่จะลงทุนกับ campaign นี้ต่อหรือไม่ <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### วัตถุประสงค์  (Objective)\n",
    "- เพื่อเรียนรู้ถึงวิธีการทำ feature extraction จาก text file เช่นการทำ stemming กับ negation handling ก่อน\n",
    "- เพื่อทำความเข้าใจและเรียนรู้ถึงวิธีการใช้ Word2Vec และ Doc2Vec ในการทำ Classification ซึ่งจะทำนายผลออกมาว่า text นั้นๆ มีผลเป็นบวกหรือเป็นลบทางอารมณ์\n",
    "- เพื่อนำความรู้ที่ได้ไปใช้ทำนายอารมณ์ของข้อความอื่นๆ\n",
    "- เพื่อเปรียบเทียบความสามารถกับวิธีอื่นๆ ทั่วไป เช่น tf-idf classifier, logistic regression หรือ random forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ลักษณะของข้อมูลที่เลือกมา\n",
    "<p>&emsp;&emsp;Large Movie Review Dataset v1.0 เป็นข้อมูลที่ประกอบด้วยรีวิวของหนังแต่ละเรื่องจาก IMDb ซึ่งเหมาะสำหรับการทำ Sentiment Classification โดยข้อมูลมีจำนวน 50000 รีวิว และใช้การแบ่งเป็น Train 70 % และ Test 30 % (ข้อมูลมีการกระจายตัวอย่างสม่ำเสมอโดยเป็น Positive review 50 % และ Negative review 50 % โดยข้อมูลที่จะถูกแบ่งเป็น Positive review จะเป็น review ที่มี rating score >=7 และ ข้อมูลที่เป็น negative review จะเป็น review ที่มี rating score <=4) โดยสามารถเข้าถึงข้อมูลได้จาก http://ai.stanford.edu/~amaas/data/sentiment/ <p><p>\n",
    "    \n",
    "### ไฟล์ที่ได้ประกอบด้วย\n",
    "- Train Set และ Test set ซึ่งมีข้อมูล Positive review และ Negative review ในแต่ละเซตข้อมูล โดยข้อมูลรีวิวถูกเก็บในรูป text files และตั้งชื่อด้วยเงื่อนไข [[id]_[rating].txt] โดย [id] เป็น unique id และ [rating] เป็น star rating ซึ่งมี rating ตั้งแต่ 1-10 เช่น [test/pos/200_8.txt] คือ รีวิวที่มี unique id 200 and star rating 8/10 from IMDb\n",
    "- Tokenized bag of words features ที่ใช้ใน Dataset นี้ได้ถูกทำไว้แล้ว เพื่อความสะดวกของผู้ที่นำไปใช้งาน ซึ่งอยู่ในใน .feat files ในรูปแบบ LIBSVM format ในแต่ละ train/test directories โดยจะเก็บในรูป sparse-vector format โดยจะเป็นการเก็บความถี่ของคำ เช่น 0:7 ใน .feat file แปลว่า คำแรกใน [imdb.vocab] พบ 7 ครั้งใน review นั้น\n",
    "\n",
    "![Example of Dataset](https://www.img.in.th/images/55bf9954485c17641335b18e6875c18c.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### วิธีการที่ใช้\n",
    "- Data >> preprocess >> Embedding Layer >> LSTM\n",
    "- Data >> preprocess >> Embedding Layer >> CNN\n",
    "- Data >> preprocess >> Embedding Layer >> CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers import Dense, Embedding, Dropout, Activation, Flatten, MaxPooling1D\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from IPython.display import SVG\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> this film was just brilliant casting <UNKNOWN> <UNKNOWN> story direction <UNKNOWN> really <UNKNOWN> the part they played and you could just imagine being there robert <UNKNOWN> is an amazing actor and now the same being director <UNKNOWN> father came from the same <UNKNOWN> <UNKNOWN> as myself so i loved the fact there was a real <UNKNOWN> with this film the <UNKNOWN> <UNKNOWN> throughout the film were great it was just brilliant so much that i <UNKNOWN> the film as soon as it was released for <UNKNOWN> and would recommend it to everyone to watch and the <UNKNOWN> <UNKNOWN> was amazing really <UNKNOWN> at the end it was so sad and you know what they say if you <UNKNOWN> at a film it must have been good and this definitely was also <UNKNOWN> to the two little <UNKNOWN> that played the <UNKNOWN> of <UNKNOWN> and paul they were just brilliant children are often left out of the <UNKNOWN> <UNKNOWN> i think because the stars that play them all <UNKNOWN> up are such a big <UNKNOWN> for the whole film but these children are amazing and should be <UNKNOWN> for what they have done don't you think the whole story was so <UNKNOWN> because it was true and was <UNKNOWN> life after all that was <UNKNOWN> with us all\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS=1000 \n",
    "INDEX_FROM=3   \n",
    "\n",
    "train,test = imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\n",
    "x_train,y_train = train\n",
    "x_test,y_test = test\n",
    "\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNKNOWN>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "print(' '.join(id_to_word[id] for id in x_train[0] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 2, 2, 65]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x_train[0][k] for k in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiles': 53540,\n",
       " \"'western\": 57510,\n",
       " 'mechanic': 9025,\n",
       " 'aggresive': 63355,\n",
       " 'pwnz': 69868,\n",
       " 'mikal': 77342,\n",
       " 'choca': 54835,\n",
       " 'rummaged': 65625,\n",
       " 'schecky': 74020,\n",
       " 'sanitizes': 57422,\n",
       " 'lybia': 56893,\n",
       " 'introduced': 1725,\n",
       " 'domineering': 15122,\n",
       " 'mechenosets': 50178,\n",
       " \"captain's\": 26444,\n",
       " 'kundera': 28727,\n",
       " \"'pretty\": 43475,\n",
       " 'sjöberg': 73829,\n",
       " 'reinterpretations': 35249,\n",
       " 'employed': 5652,\n",
       " 'decried': 63225,\n",
       " 'wmd': 53687,\n",
       " \"'on\": 30019,\n",
       " 'courtrooms': 32975,\n",
       " 'intricacy': 27931,\n",
       " 'lieutenant': 9280,\n",
       " 'illogical': 4330,\n",
       " 'celozzi': 46897,\n",
       " 'melvyn': 6597,\n",
       " \"dine's\": 63598,\n",
       " \"india'\": 66410,\n",
       " \"macarhur's\": 77457,\n",
       " \"lehar's\": 53496,\n",
       " 'fredrich': 71797,\n",
       " 'notice': 1495,\n",
       " 'calomari': 85977,\n",
       " 'slowely': 71231,\n",
       " 'chocked': 34094,\n",
       " 'sidearm': 62458,\n",
       " 'ustase': 52390,\n",
       " 'transcendence': 34367,\n",
       " 'horrormovies': 60055,\n",
       " 'cateress': 67340,\n",
       " \"carlin's\": 44275,\n",
       " 'suffocated': 40044,\n",
       " 'jumbo': 10059,\n",
       " 'reinterpretation': 79007,\n",
       " \"gorris'\": 48383,\n",
       " 'ism': 19709,\n",
       " 'blister': 69879,\n",
       " 'kana': 70379,\n",
       " 'emmanuelle': 11548,\n",
       " 'foy': 47852,\n",
       " 'fuzziness': 52124,\n",
       " 'community': 1830,\n",
       " 'paws': 27077,\n",
       " 'coholic': 73449,\n",
       " \"match'\": 69123,\n",
       " 'coleslaw': 30795,\n",
       " 'lata': 24663,\n",
       " 'octaves': 42449,\n",
       " 'eventuates': 63855,\n",
       " \"'then\": 59027,\n",
       " 'zaps': 30846,\n",
       " 'interbreed': 59165,\n",
       " 'ilene': 31104,\n",
       " 'iphigenia': 14668,\n",
       " \"i'm\": 146,\n",
       " 'nil': 24218,\n",
       " 'bidenesque': 79309,\n",
       " 'whoppie': 85767,\n",
       " 'geraldo': 32979,\n",
       " 'stick': 1231,\n",
       " 'kanno': 85266,\n",
       " 'postings': 32218,\n",
       " 'gears': 17666,\n",
       " 'furbies': 80228,\n",
       " 'slevin': 51468,\n",
       " 'deficiencies': 18598,\n",
       " 'helms': 16021,\n",
       " 'rookie': 6655,\n",
       " 'bused': 78211,\n",
       " 'gimmick': 6189,\n",
       " 'mocumentaries': 76598,\n",
       " 'yochobel': 56816,\n",
       " 'remember': 377,\n",
       " 'pima': 52422,\n",
       " 'motormouth': 64739,\n",
       " 'suxz': 70137,\n",
       " 'demonicly': 54175,\n",
       " 'sheffield': 21420,\n",
       " 'aliases': 38920,\n",
       " 'aka': 2544,\n",
       " \"80s'\": 39981,\n",
       " \"garden's\": 49526,\n",
       " \"stepmother's\": 34833,\n",
       " \"anno's\": 55758,\n",
       " \"reona's\": 54850,\n",
       " 'disquieting': 33072,\n",
       " 'pushing': 3742,\n",
       " 'bassinger': 19118,\n",
       " \"gerard's\": 25965,\n",
       " 'marty': 4897,\n",
       " 'eddie': 1812,\n",
       " \"'space\": 38929,\n",
       " 'actioners': 26540,\n",
       " 'portayal': 52848,\n",
       " 'arkoff': 77576,\n",
       " 'onmyoji': 42696,\n",
       " 'ugliest': 15961,\n",
       " 'inversely': 68971,\n",
       " 'badness': 8089,\n",
       " 'agreement': 12862,\n",
       " 'ritzig': 73573,\n",
       " 'prejudice': 5521,\n",
       " 'squeezable': 65930,\n",
       " 'friels': 25941,\n",
       " 'mccree': 80779,\n",
       " 'lilly': 11534,\n",
       " \"stacks'\": 44767,\n",
       " 'boarders': 72626,\n",
       " 'nicholas': 4577,\n",
       " 'bridegroom': 32321,\n",
       " 'cullen': 28774,\n",
       " 'degradées': 75727,\n",
       " \"1997's\": 38258,\n",
       " \"'taking\": 49688,\n",
       " \"'fill\": 74654,\n",
       " 'stridently': 35948,\n",
       " \"frankenstein's\": 20870,\n",
       " 'canary': 23795,\n",
       " 'gian': 76191,\n",
       " 'karamchand': 34495,\n",
       " 'hollanders': 56293,\n",
       " 'nukes': 30915,\n",
       " 'aldrin': 28331,\n",
       " 'mistaking': 22229,\n",
       " 'haj': 34739,\n",
       " 'rolling': 2660,\n",
       " 'paxinou': 14617,\n",
       " 'tightened': 19518,\n",
       " 'swamp': 9466,\n",
       " 'dice': 12201,\n",
       " 'winos': 78335,\n",
       " \"edwards'\": 79433,\n",
       " 'bags': 10330,\n",
       " 'dorthy': 74730,\n",
       " \"graveyard'\": 61794,\n",
       " \"isabelle's\": 36614,\n",
       " \"cure'\": 59472,\n",
       " \"rukh's\": 15468,\n",
       " \"norm's\": 85401,\n",
       " 'patria': 70324,\n",
       " 'reflexive': 22491,\n",
       " 'ailed': 69324,\n",
       " 'embassy': 17556,\n",
       " 'keoma': 57712,\n",
       " 'cine': 25012,\n",
       " 'thecoffeecoaster': 81723,\n",
       " 'championed': 31214,\n",
       " 'emulates': 34328,\n",
       " 'beheadings': 47400,\n",
       " 'barbie': 13904,\n",
       " 'deth': 27919,\n",
       " 'strokes\\x85': 57702,\n",
       " 'prohibitive': 57521,\n",
       " 'provisional': 81885,\n",
       " 'pomade': 76188,\n",
       " 'nauvoo': 35489,\n",
       " 'macmahone': 79996,\n",
       " 'concern': 4404,\n",
       " \"'fraidy\": 58077,\n",
       " 'puya': 88386,\n",
       " 'placidly': 41786,\n",
       " 'château': 20428,\n",
       " 'shake': 4464,\n",
       " 'cloaks': 88142,\n",
       " 'interstellar': 67874,\n",
       " 'mendel': 37869,\n",
       " \"'dy\": 50734,\n",
       " 'excactly': 60782,\n",
       " \"'bright\": 72169,\n",
       " 'chacotero': 80860,\n",
       " 'vanesa': 65539,\n",
       " 'sea': 2019,\n",
       " 'corvette': 33110,\n",
       " 'balaguero': 59693,\n",
       " 'malformations': 69603,\n",
       " 'graber': 56808,\n",
       " 'nonentity': 41610,\n",
       " 'differring': 74773,\n",
       " 'yussef': 54217,\n",
       " 'interdiction': 64506,\n",
       " 'anschel': 30204,\n",
       " 'cautionary': 14967,\n",
       " 'meat': 3549,\n",
       " 'recoup': 31889,\n",
       " 'hasslehoff': 43090,\n",
       " \"'cake\": 77071,\n",
       " 'breakneck': 27600,\n",
       " 'statuettes': 85306,\n",
       " \"proof'\": 67865,\n",
       " 'hadled': 60615,\n",
       " 'requires': 3443,\n",
       " 'fagging': 54558,\n",
       " 'dippie': 51725,\n",
       " 'rolfe': 36458,\n",
       " 'agers': 57915,\n",
       " 'graystone': 38623,\n",
       " 'thirtysomething': 83923,\n",
       " 'inconsequential': 10876,\n",
       " 'exterminators': 23639,\n",
       " 'clifford': 11972,\n",
       " \"pluckin'\": 52297,\n",
       " 'fishbourne': 31919,\n",
       " 'pintilie': 16604,\n",
       " 'fishermen': 29886,\n",
       " 'quicken': 70963,\n",
       " 'unheated': 53053,\n",
       " 'frasier': 24839,\n",
       " 'esai': 19482,\n",
       " 'ripped': 3314,\n",
       " 'upswept': 67576,\n",
       " 'conga': 42761,\n",
       " 'ossana': 86855,\n",
       " 'comptroller': 58863,\n",
       " 'ansonia': 71830,\n",
       " 'rana': 64448,\n",
       " \"era'\": 80713,\n",
       " 'inimitable': 14952,\n",
       " 'photoshoot': 37327,\n",
       " 'scavenging': 49494,\n",
       " \"'anywhere\": 76719,\n",
       " 'tunisia': 35460,\n",
       " 'inflated': 18507,\n",
       " 'provocations': 87719,\n",
       " 'sauciness': 85873,\n",
       " 'robotnik': 85406,\n",
       " 'surveillance': 14078,\n",
       " 'nietszchean': 58069,\n",
       " '1984ish': 41124,\n",
       " 'mickie': 38119,\n",
       " 'masted': 41049,\n",
       " 'yokel': 48020,\n",
       " 'behl': 79502,\n",
       " 'swamplands': 50358,\n",
       " 'bound': 2725,\n",
       " 'moreso': 23680,\n",
       " 'vocalize': 57136,\n",
       " 'bleating': 54567,\n",
       " 'commencement': 49811,\n",
       " 'selznick': 50303,\n",
       " 'subbed': 64757,\n",
       " 'undesirable': 24775,\n",
       " 'hostages': 12406,\n",
       " 'sickroom': 56729,\n",
       " 'externals': 64108,\n",
       " 'paquin': 13655,\n",
       " 'viscerally': 34910,\n",
       " 'recent': 1136,\n",
       " 'pols': 60151,\n",
       " 'catfight': 82988,\n",
       " 'momentary': 21441,\n",
       " 'marker': 18430,\n",
       " 'goody': 11936,\n",
       " 'mah': 36655,\n",
       " 'bandaged': 33525,\n",
       " 'perceptions': 18316,\n",
       " 'maojlovic': 56947,\n",
       " 'acturly': 85710,\n",
       " 'dobb': 38318,\n",
       " 'superfluos': 71679,\n",
       " 'hangman': 75689,\n",
       " 'mobility': 25384,\n",
       " 'precociousness': 54548,\n",
       " 'koon': 68423,\n",
       " \"mordrid'\": 41966,\n",
       " 'tous': 54603,\n",
       " 'inert': 15924,\n",
       " \"elliott's\": 20170,\n",
       " 'contempory': 57803,\n",
       " 'caprica': 7834,\n",
       " 'intertwines': 38964,\n",
       " 'bick': 80855,\n",
       " 'tremendously': 6529,\n",
       " 'useful': 4507,\n",
       " 'korsmo': 26595,\n",
       " 'chori': 31808,\n",
       " 'horrendousness': 60683,\n",
       " 'jog': 22102,\n",
       " 'shipload': 49297,\n",
       " 'thou': 12185,\n",
       " 'institution': 5924,\n",
       " 'jefferies': 81827,\n",
       " 'inflicts': 25944,\n",
       " 'massacred': 18710,\n",
       " 'uniformly': 5960,\n",
       " 'racks': 21769,\n",
       " \"1922'\": 61711,\n",
       " 'qvc': 49386,\n",
       " \"'duty'\": 69907,\n",
       " 'barsi': 23201,\n",
       " \"watch'\": 61473,\n",
       " 'attends': 13157,\n",
       " 'muckraker': 57524,\n",
       " 'garlands': 56624,\n",
       " 'sleuthing': 27656,\n",
       " 'messiness': 28542,\n",
       " 'nickelodean': 83476,\n",
       " 'expectant': 20025,\n",
       " 'casting\\x85': 81499,\n",
       " 'skulks': 77173,\n",
       " 'prospered': 39180,\n",
       " 'rutger': 21708,\n",
       " 'vickers': 41121,\n",
       " 'immature': 5832,\n",
       " 'yearm': 70739,\n",
       " 'loud': 1292,\n",
       " 'wear': 2637,\n",
       " 'pilfered': 50283,\n",
       " 'clawed': 39134,\n",
       " 'disconcerted': 37719,\n",
       " 'hunnicutt': 41480,\n",
       " 'whitewash': 24456,\n",
       " 'cutaway': 31828,\n",
       " 'mouskouri': 65360,\n",
       " 'watterman': 50519,\n",
       " 'loosly': 41044,\n",
       " 'leviticus': 73355,\n",
       " 'goelz': 79057,\n",
       " 'nrk': 52730,\n",
       " 'sabra': 50508,\n",
       " 'buffed': 46566,\n",
       " 'markland': 72836,\n",
       " 'matiko': 48398,\n",
       " 'wournow': 74071,\n",
       " 'refered': 65124,\n",
       " 'rstj': 79044,\n",
       " 'guardano': 65241,\n",
       " 'shortest': 22324,\n",
       " \"dru's\": 60549,\n",
       " 'kibitz': 58225,\n",
       " 'disappointment': 1387,\n",
       " 'toro': 9511,\n",
       " 'rapist': 5749,\n",
       " 'freebird': 17190,\n",
       " 'satterfield': 58680,\n",
       " 'smartaleck': 50981,\n",
       " 'dribbles': 77561,\n",
       " 'impulsively': 39925,\n",
       " 'unsettlingly': 39915,\n",
       " 'rofl': 65441,\n",
       " 'dwars': 72825,\n",
       " 'animates': 27447,\n",
       " 'fumiko': 48788,\n",
       " 'slags': 69313,\n",
       " 'pavlov': 34204,\n",
       " 'high': 312,\n",
       " \"eibon'\": 81096,\n",
       " \"'patton'\": 76304,\n",
       " \"karin's\": 61901,\n",
       " 'ogi': 59497,\n",
       " 'crabbe': 50830,\n",
       " 'cussler': 65737,\n",
       " \"'07\": 34097,\n",
       " 'mesmorizingly': 83633,\n",
       " 'l': 2014,\n",
       " 'hoskins': 20880,\n",
       " 'reachable': 66290,\n",
       " 'tsing': 15028,\n",
       " 'saga': 4296,\n",
       " \"'poets\": 84885,\n",
       " 'menopuasal': 83360,\n",
       " 'incrementally': 52299,\n",
       " 'slasherville': 78212,\n",
       " 'norwegian': 9495,\n",
       " \"'policial'\": 50613,\n",
       " 'pinko': 61331,\n",
       " \"graduate'\": 42535,\n",
       " 'ripley': 10644,\n",
       " 'macguyver': 69796,\n",
       " 'moans': 17034,\n",
       " \"coltrane's\": 55106,\n",
       " 'poopers': 55009,\n",
       " \"1939's\": 42929,\n",
       " 'contributions': 11926,\n",
       " 'from': 39,\n",
       " 'wilkins': 35613,\n",
       " 'cattivi': 57828,\n",
       " \"'lost'\": 22509,\n",
       " 'manged': 50760,\n",
       " 'lest': 11889,\n",
       " 'dicenzo': 46353,\n",
       " 'intermingle': 42816,\n",
       " 'chim': 30750,\n",
       " 'eats': 5774,\n",
       " 'boyle': 5830,\n",
       " 'danze': 53936,\n",
       " 'hilaraious': 78963,\n",
       " 'premiered': 8433,\n",
       " 'frencified': 68009,\n",
       " 'agonized': 29207,\n",
       " 'dunk': 15000,\n",
       " \"kings'\": 53873,\n",
       " 'setton': 48633,\n",
       " 'majo': 52148,\n",
       " 'meteorologist': 52060,\n",
       " 'insightfully': 29266,\n",
       " 'exiting': 14203,\n",
       " 'overdramatizes': 59080,\n",
       " 'brunda': 65293,\n",
       " 'chal': 46688,\n",
       " 'cheezy': 13698,\n",
       " 'sr': 9569,\n",
       " 'audrie': 26246,\n",
       " 'pago': 49793,\n",
       " 'hedgrowed': 59714,\n",
       " 'gouts': 70966,\n",
       " 'listed': 3576,\n",
       " 'velociraptor': 48775,\n",
       " 'bend': 5899,\n",
       " 'guano': 28251,\n",
       " 'suge': 85941,\n",
       " 'farmhouse': 24152,\n",
       " 'rylance': 52452,\n",
       " 'sílvia': 56337,\n",
       " 'squeakiest': 61895,\n",
       " 'diddley': 53430,\n",
       " 'corby': 33011,\n",
       " \"imagining'\": 50729,\n",
       " 'monotone': 10525,\n",
       " 'thornway': 19720,\n",
       " \"y'see\": 80738,\n",
       " 'verna': 40342,\n",
       " 'quivers': 86492,\n",
       " 'overtops': 73655,\n",
       " 'stutter': 25257,\n",
       " '3\\x85': 52199,\n",
       " 'mannered': 7573,\n",
       " 'eyeless': 60977,\n",
       " \"mankind's\": 22521,\n",
       " 'counted': 9234,\n",
       " \"'protesting\": 50049,\n",
       " 'forgiven': 6053,\n",
       " 'harbinger': 30046,\n",
       " 'preliminaries': 42563,\n",
       " 'schuer': 60093,\n",
       " 'bulb': 11951,\n",
       " 'off': 125,\n",
       " 'dhéry': 55927,\n",
       " 'upsmanship': 67923,\n",
       " 'unfairly': 9996,\n",
       " 'varieties': 45107,\n",
       " 'raj': 5883,\n",
       " 'tuxedoed': 81645,\n",
       " 'slit': 12337,\n",
       " \"shindler's\": 61936,\n",
       " 'barrister': 18154,\n",
       " 'chandramukhi': 48976,\n",
       " 'looks': 272,\n",
       " 'viewing': 829,\n",
       " \"harris's\": 24462,\n",
       " 'tawnee': 34918,\n",
       " 'stating': 7126,\n",
       " 'overthrowing': 81247,\n",
       " 'cardiovascular': 67388,\n",
       " 'dumblaine': 76353,\n",
       " \"let's\": 903,\n",
       " 'heebie': 41095,\n",
       " \"christine's\": 21581,\n",
       " 'salivating': 35633,\n",
       " 'hun': 25777,\n",
       " \"liza's\": 50483,\n",
       " 'guru': 8990,\n",
       " 'commercisliation': 57361,\n",
       " \"plumtree's\": 77217,\n",
       " 'insurgency': 34209,\n",
       " 'inept': 2797,\n",
       " \"meighan's\": 75462,\n",
       " \"have'\": 66487,\n",
       " \"duff's\": 36116,\n",
       " 'sicko': 21181,\n",
       " 'manikin': 73969,\n",
       " \"barrels'\": 49198,\n",
       " 'othenin': 47579,\n",
       " 'balboa': 48464,\n",
       " 'encroachment': 44162,\n",
       " 'mit': 53310,\n",
       " \"\\x91insignificance'\": 88413,\n",
       " \"bearings'\": 88561,\n",
       " 'warpaint': 85372,\n",
       " \"'unfunny'\": 52132,\n",
       " 'dirtmaster': 79763,\n",
       " 'slained': 69509,\n",
       " \"civilisation's\": 86079,\n",
       " 'unflinchingly': 39599,\n",
       " 'forfend': 69619,\n",
       " 'willow': 20384,\n",
       " 'bmi': 77385,\n",
       " 'cute': 1036,\n",
       " 'trips': 10558,\n",
       " 'receptionist': 21458,\n",
       " 'ourselves': 3147,\n",
       " 'cabell': 23620,\n",
       " 'kwok': 38202,\n",
       " 'numbing': 6547,\n",
       " 'ritterkreuz': 77346,\n",
       " \"beyonce's\": 43667,\n",
       " 'towering': 17576,\n",
       " 'shylock': 59278,\n",
       " 'contemplative': 20333,\n",
       " 'creely': 79747,\n",
       " 'cinematograph': 28028,\n",
       " 'you´ll': 47611,\n",
       " 'chewbaka': 73461,\n",
       " 'enchilada': 49188,\n",
       " 'scarcity': 28483,\n",
       " 'iaac': 84358,\n",
       " 'guayabera': 59419,\n",
       " \"'relationship'\": 72897,\n",
       " 'walthal': 58201,\n",
       " 'directory': 35378,\n",
       " 'playoffs': 29332,\n",
       " \"euro's\": 56978,\n",
       " 'starfish': 84177,\n",
       " 'hino': 81490,\n",
       " 'giovinazzo': 86395,\n",
       " 'zig': 50812,\n",
       " 'quotient': 15945,\n",
       " '345': 23550,\n",
       " 'marlow': 38443,\n",
       " \"'inspiration'\": 60823,\n",
       " \"crucifix's\": 79580,\n",
       " 'mud': 7934,\n",
       " 'roland': 11292,\n",
       " 'rerun': 14307,\n",
       " 'zealous': 19223,\n",
       " 'chief': 2297,\n",
       " 'pelicule': 77279,\n",
       " 'thew': 38749,\n",
       " 'howdy': 55304,\n",
       " 'naughtily': 82182,\n",
       " 'borgnine': 18093,\n",
       " 'fiendish': 19083,\n",
       " 'weaves': 9973,\n",
       " 'capitulated': 77756,\n",
       " 'parodic': 42034,\n",
       " 'sulfate\\x85': 67352,\n",
       " \"magician's\": 35471,\n",
       " \"zeppelin's\": 82476,\n",
       " 'quran': 40427,\n",
       " 'occassionally': 39384,\n",
       " 'gabel': 26728,\n",
       " \"ce3k'\": 79885,\n",
       " '1917': 20105,\n",
       " 'sharat': 62652,\n",
       " 'tais': 60761,\n",
       " 'pratfalls': 19196,\n",
       " 'outlandish': 7497,\n",
       " 'jox': 13329,\n",
       " 'sometines': 64406,\n",
       " \"kmc's\": 61614,\n",
       " 'saurious': 61363,\n",
       " 'reamke': 75359,\n",
       " 'subterfuge': 21665,\n",
       " 'donig': 70097,\n",
       " 'sickingly': 46896,\n",
       " \"samson's\": 55208,\n",
       " 'slaps': 12606,\n",
       " \"four'\": 53575,\n",
       " 'shelling': 31956,\n",
       " 'keil': 67381,\n",
       " 'severe': 4376,\n",
       " 'dependably': 44378,\n",
       " 'zappruder': 42497,\n",
       " 'technicolor': 5107,\n",
       " \"austria's\": 43061,\n",
       " 'painfull': 54748,\n",
       " 'mall': 4482,\n",
       " 'conspiratorial': 35579,\n",
       " 'ribbons': 19712,\n",
       " 'laundry': 11416,\n",
       " 'narration': 2559,\n",
       " 'motocross': 84347,\n",
       " 'nekked': 83173,\n",
       " \"watros'\": 57781,\n",
       " 'ernest': 8341,\n",
       " 'exult': 61643,\n",
       " \"nations'\": 46971,\n",
       " \"palette's\": 60746,\n",
       " 'mellon': 57644,\n",
       " 'piovani': 29225,\n",
       " 'outlawed': 23192,\n",
       " 'giannini': 18101,\n",
       " 'limburger': 78195,\n",
       " 'impassivity': 80847,\n",
       " 'flimsily': 46060,\n",
       " \"'smooth\": 46569,\n",
       " 'salutary': 69260,\n",
       " 'mimieux': 26625,\n",
       " 'chip': 8175,\n",
       " 'androids': 33115,\n",
       " 'holes': 1512,\n",
       " 'aimlessness': 79552,\n",
       " \"adama'\": 87844,\n",
       " 'gooks': 75521,\n",
       " 'anniversaries': 52130,\n",
       " 'obstructive': 58965,\n",
       " \"oxford's\": 58868,\n",
       " 'workhorse': 42167,\n",
       " 'solid': 1156,\n",
       " 'gangreen': 82567,\n",
       " 'afterword': 66610,\n",
       " 'organizations': 17050,\n",
       " 'eminating': 57522,\n",
       " '80yr': 62988,\n",
       " 'minnesota': 18599,\n",
       " 'provide': 1754,\n",
       " 'bogglingly': 27675,\n",
       " 'allthewhile': 55411,\n",
       " 'quietus': 60827,\n",
       " '819': 65790,\n",
       " 'gimped': 48796,\n",
       " 'biographys': 81681,\n",
       " 'skywalker': 9766,\n",
       " 'mckay': 17166,\n",
       " 'trumpery': 67238,\n",
       " 'waisting': 53979,\n",
       " 'supers': 87867,\n",
       " 'render': 9564,\n",
       " 'wuss': 17297,\n",
       " 'dangerfield': 14133,\n",
       " 'lightened': 28108,\n",
       " 'itz': 84429,\n",
       " 'dh': 26819,\n",
       " 'thinkthey': 61290,\n",
       " 'krick': 41449,\n",
       " 'outfits': 5409,\n",
       " 'theory': 2604,\n",
       " 'deathrow': 49229,\n",
       " 'espy': 56705,\n",
       " 'mullet': 13903,\n",
       " 'cept': 43038,\n",
       " 'latin': 4556,\n",
       " 'fl': 33296,\n",
       " 'synanomess': 52519,\n",
       " 'sexagenarians': 73258,\n",
       " 'naruto': 36673,\n",
       " 'lyrically': 56062,\n",
       " \"napier's\": 46105,\n",
       " 'calming': 39807,\n",
       " 'spoils': 14988,\n",
       " 'monasteries': 66521,\n",
       " 'estrogen': 44146,\n",
       " 'wicker': 9941,\n",
       " 'videostore': 22934,\n",
       " 'polnareff': 83483,\n",
       " 'pragmatics': 76206,\n",
       " 'wallpapers': 38135,\n",
       " 'murders': 1496,\n",
       " 'reccommend': 61324,\n",
       " 'transfused': 38331,\n",
       " 'comforting': 12992,\n",
       " 'teeter': 57025,\n",
       " 'gravedigger': 19671,\n",
       " 'pander': 18702,\n",
       " 'hopped': 29798,\n",
       " 'jealousies': 32677,\n",
       " 'osiric': 56905,\n",
       " \"z's\": 76280,\n",
       " 'adorning': 48120,\n",
       " 'figured': 2626,\n",
       " 'acquiring': 24455,\n",
       " 'unionist': 73663,\n",
       " 'farmworker': 65616,\n",
       " 'assistance': 7988,\n",
       " 'saintly': 27158,\n",
       " 'kenyon': 24783,\n",
       " 'nappies': 47139,\n",
       " 'gospel': 11284,\n",
       " 'babtise': 76295,\n",
       " 'juliet': 6339,\n",
       " 'elapsed': 38326,\n",
       " \"val's\": 40256,\n",
       " 'unconcious': 72504,\n",
       " 'china': 2714,\n",
       " 'nonactor': 62094,\n",
       " \"deviants'\": 64999,\n",
       " 'sondra': 7353,\n",
       " \"'doping'\": 81796,\n",
       " \"'rangi'\": 68704,\n",
       " 'photochemical': 61270,\n",
       " 'fallacies': 28429,\n",
       " \"hudson's\": 19413,\n",
       " 'heckling': 32593,\n",
       " 'tellytubbies': 84884,\n",
       " 'ahead': 1404,\n",
       " \"widow's\": 28058,\n",
       " 'montano': 24609,\n",
       " 'missive': 54400,\n",
       " 'unaired': 31392,\n",
       " 'eyeboy': 67261,\n",
       " \"is'\": 25671,\n",
       " 'symptomatic': 28488,\n",
       " '529': 78296,\n",
       " 'scrumptious': 32169,\n",
       " 'idk': 34386,\n",
       " 'switched': 6699,\n",
       " 'yoshiaki': 43825,\n",
       " 'pies': 19025,\n",
       " 'halfway': 2807,\n",
       " 'distance': 3744,\n",
       " \"literature's\": 49312,\n",
       " 'request': 9468,\n",
       " 'unsweaty': 53546,\n",
       " 'medichlorians': 58583,\n",
       " \"sanjiv's\": 63652,\n",
       " 'marmaduke': 41268,\n",
       " 'mismatches': 77288,\n",
       " 'ambience': 36310,\n",
       " 'nickel': 32109,\n",
       " 'trans': 20647,\n",
       " 'deterioration': 17882,\n",
       " 'worshipful': 55473,\n",
       " \"'trick\": 86879,\n",
       " 'entrap': 45719,\n",
       " 'francs': 48388,\n",
       " \"weird's\": 60996,\n",
       " 'luring': 22801,\n",
       " \"ape'\": 78357,\n",
       " 'heartland': 15414,\n",
       " 'chains': 9653,\n",
       " 'admonition': 67318,\n",
       " 'stagebound': 32248,\n",
       " 'understorey': 74933,\n",
       " \"contestant's\": 44754,\n",
       " 'mangal': 81534,\n",
       " 'jackies': 34442,\n",
       " 'girth': 28004,\n",
       " 'divyashakti': 55783,\n",
       " 'rejections': 83593,\n",
       " 'koun': 38578,\n",
       " \"brimmer's\": 66267,\n",
       " 'hissy': 28046,\n",
       " 'ultimtum': 56118,\n",
       " 'reems': 33551,\n",
       " 'lilting': 46324,\n",
       " 'markes': 85275,\n",
       " 'resolutions': 22242,\n",
       " 'vet': 8542,\n",
       " \"springsteen's\": 47936,\n",
       " 'decor': 14437,\n",
       " 'orbiting': 28340,\n",
       " 'casavates': 83946,\n",
       " 'secretive': 15005,\n",
       " 'daring': 3922,\n",
       " 'fumbles': 39838,\n",
       " \"at'\": 83852,\n",
       " 'lensed': 24692,\n",
       " 'gathered': 8676,\n",
       " 'kavanagh': 41839,\n",
       " 'deploy': 36563,\n",
       " 'underlie': 74800,\n",
       " 'zaniacs': 52133,\n",
       " 'bugle': 27630,\n",
       " 'morone': 56758,\n",
       " 'surnow': 77846,\n",
       " \"eyre'\": 27659,\n",
       " 'wills': 12720,\n",
       " 'alessandra': 30695,\n",
       " 'subsequent': 3691,\n",
       " 'charlton': 5910,\n",
       " 'velociraptors': 49041,\n",
       " 'urging': 17285,\n",
       " 'yawns': 23648,\n",
       " 'fannin': 67515,\n",
       " 'abnormality': 65147,\n",
       " 'edinburugh': 63357,\n",
       " 'hangdog': 45165,\n",
       " 'outlive': 62294,\n",
       " 'gob': 44414,\n",
       " 'risible': 19203,\n",
       " 'demons': 2624,\n",
       " 'megawatt': 62875,\n",
       " 'lectern': 43869,\n",
       " 'problem': 439,\n",
       " 'nikolaj': 25495,\n",
       " \"'flash'\": 56196,\n",
       " 'monitors': 13438,\n",
       " \"'spacecamp'\": 64948,\n",
       " 'orchids': 25859,\n",
       " 'apologetic': 35247,\n",
       " 'validating': 51972,\n",
       " 'extraterrestrials': 41523,\n",
       " 'qc': 66565,\n",
       " 'brownlow': 61696,\n",
       " 'diviner': 28599,\n",
       " \"dancy's\": 54955,\n",
       " 'goetter': 79537,\n",
       " 'nuttery': 72769,\n",
       " 'onslow': 66128,\n",
       " 'indecision': 33177,\n",
       " 'coy': 17365,\n",
       " \"'introducing\": 53591,\n",
       " 'endor': 15008,\n",
       " 'vainglorious': 55597,\n",
       " 'hrabal': 62748,\n",
       " 'mfer': 70663,\n",
       " 'vestment': 52174,\n",
       " 'accusatory': 51277,\n",
       " 'ornaments': 31351,\n",
       " \"galaxy'\": 86296,\n",
       " 'bbc': 2986,\n",
       " 'jayaraj': 58413,\n",
       " \"reloaded'\": 41081,\n",
       " 'sidetracking': 82859,\n",
       " 'raye': 20852,\n",
       " 'embattled': 31686,\n",
       " 'clinique': 56285,\n",
       " 'shepherdess': 71022,\n",
       " 'rotne': 79032,\n",
       " 'asap': 21235,\n",
       " 'fixit': 30007,\n",
       " 'spetznatz': 49803,\n",
       " 'mull': 54726,\n",
       " 'garp': 36846,\n",
       " 'bowled': 26233,\n",
       " 'traces': 11426,\n",
       " 'borchers': 59231,\n",
       " 'avert': 22108,\n",
       " \"tamblyn's\": 78004,\n",
       " \"sable's\": 53294,\n",
       " 'tenderhearted': 61676,\n",
       " \"marley's\": 38394,\n",
       " 'vaginal': 44306,\n",
       " 'earned': 4300,\n",
       " '\\x08\\x08\\x08\\x08a': 68896,\n",
       " 'apache': 34651,\n",
       " 'sublimated': 37870,\n",
       " \"moranis's\": 67884,\n",
       " '¨calling': 65026,\n",
       " \"sondra's\": 37294,\n",
       " 'nan': 17984,\n",
       " 'backbeat': 78255,\n",
       " 'mendez': 17246,\n",
       " 'kawajiri': 75142,\n",
       " \"wallach's\": 47320,\n",
       " \"50's\": 3707,\n",
       " \"cup'\": 68670,\n",
       " 'groupie': 18267,\n",
       " 'pulling': 3661,\n",
       " 'sloths': 49539,\n",
       " \"'buy\": 45330,\n",
       " 'summaries': 23589,\n",
       " 'themselves': 533,\n",
       " 'stark': 5315,\n",
       " 'kosti': 82036,\n",
       " 'chushingura': 74740,\n",
       " 'toooooo': 47306,\n",
       " \"bruhl's\": 76226,\n",
       " 'clair': 13861,\n",
       " 'dondaro': 67796,\n",
       " 'brights': 63818,\n",
       " 'coped': 41073,\n",
       " 'overturned': 46522,\n",
       " 'lattices': 54853,\n",
       " 'menendez': 69747,\n",
       " 'jiri': 32473,\n",
       " 'cheyenne': 16277,\n",
       " 'choreographies': 76863,\n",
       " 'unassuming': 19308,\n",
       " 'renewal': 39725,\n",
       " 'batzella': 72953,\n",
       " 'retelling': 9214,\n",
       " 'jordowsky': 87496,\n",
       " 'wongo': 68056,\n",
       " \"cbs's\": 43555,\n",
       " 'remind': 3027,\n",
       " 'harasses': 36742,\n",
       " 'deteste': 56096,\n",
       " 'shaggy': 7312,\n",
       " 'hawtrey': 35459,\n",
       " 'province': 14221,\n",
       " 'intimidation': 86208,\n",
       " 'khoma': 76982,\n",
       " 'scatological': 26247,\n",
       " 'paramount': 5510,\n",
       " 'sinuses': 37728,\n",
       " 'felliniesque': 79322,\n",
       " 'peterson': 9201,\n",
       " 'gogu': 69308,\n",
       " \"harpo's\": 40834,\n",
       " 'largely': 2256,\n",
       " 'copout': 42576,\n",
       " 'ferhan': 70540,\n",
       " 'hefty': 15330,\n",
       " 'propagandized': 44448,\n",
       " 'commercialism': 21232,\n",
       " \"camel's\": 48195,\n",
       " 'intend': 8001,\n",
       " \"raja's\": 85074,\n",
       " 'herr': 21102,\n",
       " 'scolding': 31976,\n",
       " 'yosemite': 51268,\n",
       " 'humanely': 49561,\n",
       " 'sikes': 21500,\n",
       " \"go'\": 31498,\n",
       " \"adventures'\": 82176,\n",
       " 'magnani': 46642,\n",
       " \"'discovering\": 86512,\n",
       " 'wiest': 39586,\n",
       " 'gams': 81007,\n",
       " 'fascists': 19255,\n",
       " 'homesickness': 49503,\n",
       " 'striking': 3347,\n",
       " \"chandrasekhar's\": 75867,\n",
       " 'rigg': 20914,\n",
       " 'forslani': 57447,\n",
       " 'mezzo': 48944,\n",
       " \"vampire's\": 26443,\n",
       " 'bedeviled': 37151,\n",
       " 'ghent': 42397,\n",
       " 'downstairs': 15089,\n",
       " 'salvation': 6317,\n",
       " 'rifkin': 35183,\n",
       " \"squads'\": 54587,\n",
       " 'uneasiness': 28932,\n",
       " 'proprietress': 49767,\n",
       " 'plastic': 3161,\n",
       " 'antevleva': 82715,\n",
       " 'misfire': 9849,\n",
       " 'founders': 38057,\n",
       " 'shortcut': 41374,\n",
       " 'uninvited': 29351,\n",
       " 'dickman': 45268,\n",
       " \"v'\": 80146,\n",
       " 'footloose': 45008,\n",
       " 'À': 59817,\n",
       " 'liquids': 48505,\n",
       " 'interresting': 67217,\n",
       " 'africans': 11165,\n",
       " \"apc's\": 86014,\n",
       " 'sunglass': 68178,\n",
       " 'retitled': 19000,\n",
       " \"layman's\": 50616,\n",
       " 'fruitfully': 75333,\n",
       " 'nonreligious': 67552,\n",
       " '11m': 70360,\n",
       " 'celebi': 16182,\n",
       " \"'tubes\": 77554,\n",
       " 'breuer': 62715,\n",
       " 'towards': 949,\n",
       " 'external': 11436,\n",
       " 'decorous': 79126,\n",
       " 'unnoticeable': 51487,\n",
       " 'jurisdiction': 23088,\n",
       " \"garzon's\": 83827,\n",
       " 'crusading': 28209,\n",
       " \"jennifer's\": 18314,\n",
       " 'leggy': 29482,\n",
       " 'batmite': 62954,\n",
       " 'slowmotion': 61759,\n",
       " 'cinemagic': 28935,\n",
       " 'shrunk': 16241,\n",
       " 'nunchaku': 56730,\n",
       " 'mordem': 54562,\n",
       " 'victis': 85160,\n",
       " 'uncurbed': 56713,\n",
       " \"prague'\": 38279,\n",
       " \"'scare'\": 47190,\n",
       " \"early's\": 22396,\n",
       " 'bissau': 47232,\n",
       " 'camp': 1250,\n",
       " 'devised': 13592,\n",
       " 'cockfight': 49652,\n",
       " 'shuddering': 24300,\n",
       " 'appalling': 3577,\n",
       " 'pixilated': 52146,\n",
       " 'strangulation': 21468,\n",
       " 'manned': 26815,\n",
       " 'deathtrap': 7770,\n",
       " 'gumshoes': 84853,\n",
       " 'whoever': 2500,\n",
       " 'greasy': 15009,\n",
       " 'whaling': 19642,\n",
       " 'shipwrecked': 27321,\n",
       " 'hush': 15075,\n",
       " 'uncomically': 64462,\n",
       " 'laff': 41265,\n",
       " 'bogie': 19015,\n",
       " 'coogan': 30875,\n",
       " 'theologically': 72135,\n",
       " 'tirades': 42495,\n",
       " 'stefan': 10159,\n",
       " 'sore': 8891,\n",
       " 'convex': 69091,\n",
       " \"giallo's\": 27666,\n",
       " 'yôko': 51136,\n",
       " \"'domino'\": 42388,\n",
       " \"'twist'\": 21486,\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>',\n",
       " 1: '<START>',\n",
       " 2: '<UNKNOWN>',\n",
       " 4: 'the',\n",
       " 5: 'and',\n",
       " 6: 'a',\n",
       " 7: 'of',\n",
       " 8: 'to',\n",
       " 9: 'is',\n",
       " 10: 'br',\n",
       " 11: 'in',\n",
       " 12: 'it',\n",
       " 13: 'i',\n",
       " 14: 'this',\n",
       " 15: 'that',\n",
       " 16: 'was',\n",
       " 17: 'as',\n",
       " 18: 'for',\n",
       " 19: 'with',\n",
       " 20: 'movie',\n",
       " 21: 'but',\n",
       " 22: 'film',\n",
       " 23: 'on',\n",
       " 24: 'not',\n",
       " 25: 'you',\n",
       " 26: 'are',\n",
       " 27: 'his',\n",
       " 28: 'have',\n",
       " 29: 'he',\n",
       " 30: 'be',\n",
       " 31: 'one',\n",
       " 32: 'all',\n",
       " 33: 'at',\n",
       " 34: 'by',\n",
       " 35: 'an',\n",
       " 36: 'they',\n",
       " 37: 'who',\n",
       " 38: 'so',\n",
       " 39: 'from',\n",
       " 40: 'like',\n",
       " 41: 'her',\n",
       " 42: 'or',\n",
       " 43: 'just',\n",
       " 44: 'about',\n",
       " 45: \"it's\",\n",
       " 46: 'out',\n",
       " 47: 'has',\n",
       " 48: 'if',\n",
       " 49: 'some',\n",
       " 50: 'there',\n",
       " 51: 'what',\n",
       " 52: 'good',\n",
       " 53: 'more',\n",
       " 54: 'when',\n",
       " 55: 'very',\n",
       " 56: 'up',\n",
       " 57: 'no',\n",
       " 58: 'time',\n",
       " 59: 'she',\n",
       " 60: 'even',\n",
       " 61: 'my',\n",
       " 62: 'would',\n",
       " 63: 'which',\n",
       " 64: 'only',\n",
       " 65: 'story',\n",
       " 66: 'really',\n",
       " 67: 'see',\n",
       " 68: 'their',\n",
       " 69: 'had',\n",
       " 70: 'can',\n",
       " 71: 'were',\n",
       " 72: 'me',\n",
       " 73: 'well',\n",
       " 74: 'than',\n",
       " 75: 'we',\n",
       " 76: 'much',\n",
       " 77: 'been',\n",
       " 78: 'bad',\n",
       " 79: 'get',\n",
       " 80: 'will',\n",
       " 81: 'do',\n",
       " 82: 'also',\n",
       " 83: 'into',\n",
       " 84: 'people',\n",
       " 85: 'other',\n",
       " 86: 'first',\n",
       " 87: 'great',\n",
       " 88: 'because',\n",
       " 89: 'how',\n",
       " 90: 'him',\n",
       " 91: 'most',\n",
       " 92: \"don't\",\n",
       " 93: 'made',\n",
       " 94: 'its',\n",
       " 95: 'then',\n",
       " 96: 'way',\n",
       " 97: 'make',\n",
       " 98: 'them',\n",
       " 99: 'too',\n",
       " 100: 'could',\n",
       " 101: 'any',\n",
       " 102: 'movies',\n",
       " 103: 'after',\n",
       " 104: 'think',\n",
       " 105: 'characters',\n",
       " 106: 'watch',\n",
       " 107: 'two',\n",
       " 108: 'films',\n",
       " 109: 'character',\n",
       " 110: 'seen',\n",
       " 111: 'many',\n",
       " 112: 'being',\n",
       " 113: 'life',\n",
       " 114: 'plot',\n",
       " 115: 'never',\n",
       " 116: 'acting',\n",
       " 117: 'little',\n",
       " 118: 'best',\n",
       " 119: 'love',\n",
       " 120: 'over',\n",
       " 121: 'where',\n",
       " 122: 'did',\n",
       " 123: 'show',\n",
       " 124: 'know',\n",
       " 125: 'off',\n",
       " 126: 'ever',\n",
       " 127: 'does',\n",
       " 128: 'better',\n",
       " 129: 'your',\n",
       " 130: 'end',\n",
       " 131: 'still',\n",
       " 132: 'man',\n",
       " 133: 'here',\n",
       " 134: 'these',\n",
       " 135: 'say',\n",
       " 136: 'scene',\n",
       " 137: 'while',\n",
       " 138: 'why',\n",
       " 139: 'scenes',\n",
       " 140: 'go',\n",
       " 141: 'such',\n",
       " 142: 'something',\n",
       " 143: 'through',\n",
       " 144: 'should',\n",
       " 145: 'back',\n",
       " 146: \"i'm\",\n",
       " 147: 'real',\n",
       " 148: 'those',\n",
       " 149: 'watching',\n",
       " 150: 'now',\n",
       " 151: 'though',\n",
       " 152: \"doesn't\",\n",
       " 153: 'years',\n",
       " 154: 'old',\n",
       " 155: 'thing',\n",
       " 156: 'actors',\n",
       " 157: 'work',\n",
       " 158: '10',\n",
       " 159: 'before',\n",
       " 160: 'another',\n",
       " 161: \"didn't\",\n",
       " 162: 'new',\n",
       " 163: 'funny',\n",
       " 164: 'nothing',\n",
       " 165: 'actually',\n",
       " 166: 'makes',\n",
       " 167: 'director',\n",
       " 168: 'look',\n",
       " 169: 'find',\n",
       " 170: 'going',\n",
       " 171: 'few',\n",
       " 172: 'same',\n",
       " 173: 'part',\n",
       " 174: 'again',\n",
       " 175: 'every',\n",
       " 176: 'lot',\n",
       " 177: 'cast',\n",
       " 178: 'us',\n",
       " 179: 'quite',\n",
       " 180: 'down',\n",
       " 181: 'want',\n",
       " 182: 'world',\n",
       " 183: 'things',\n",
       " 184: 'pretty',\n",
       " 185: 'young',\n",
       " 186: 'seems',\n",
       " 187: 'around',\n",
       " 188: 'got',\n",
       " 189: 'horror',\n",
       " 190: 'however',\n",
       " 191: \"can't\",\n",
       " 192: 'fact',\n",
       " 193: 'take',\n",
       " 194: 'big',\n",
       " 195: 'enough',\n",
       " 196: 'long',\n",
       " 197: 'thought',\n",
       " 198: \"that's\",\n",
       " 199: 'both',\n",
       " 200: 'between',\n",
       " 201: 'series',\n",
       " 202: 'give',\n",
       " 203: 'may',\n",
       " 204: 'original',\n",
       " 205: 'own',\n",
       " 206: 'action',\n",
       " 207: \"i've\",\n",
       " 208: 'right',\n",
       " 209: 'without',\n",
       " 210: 'always',\n",
       " 211: 'times',\n",
       " 212: 'comedy',\n",
       " 213: 'point',\n",
       " 214: 'gets',\n",
       " 215: 'must',\n",
       " 216: 'come',\n",
       " 217: 'role',\n",
       " 218: \"isn't\",\n",
       " 219: 'saw',\n",
       " 220: 'almost',\n",
       " 221: 'interesting',\n",
       " 222: 'least',\n",
       " 223: 'family',\n",
       " 224: 'done',\n",
       " 225: \"there's\",\n",
       " 226: 'whole',\n",
       " 227: 'bit',\n",
       " 228: 'music',\n",
       " 229: 'script',\n",
       " 230: 'far',\n",
       " 231: 'making',\n",
       " 232: 'guy',\n",
       " 233: 'anything',\n",
       " 234: 'minutes',\n",
       " 235: 'feel',\n",
       " 236: 'last',\n",
       " 237: 'since',\n",
       " 238: 'might',\n",
       " 239: 'performance',\n",
       " 240: \"he's\",\n",
       " 241: '2',\n",
       " 242: 'probably',\n",
       " 243: 'kind',\n",
       " 244: 'am',\n",
       " 245: 'away',\n",
       " 246: 'yet',\n",
       " 247: 'rather',\n",
       " 248: 'tv',\n",
       " 249: 'worst',\n",
       " 250: 'girl',\n",
       " 251: 'day',\n",
       " 252: 'sure',\n",
       " 253: 'fun',\n",
       " 254: 'hard',\n",
       " 255: 'woman',\n",
       " 256: 'played',\n",
       " 257: 'each',\n",
       " 258: 'found',\n",
       " 259: 'anyone',\n",
       " 260: 'having',\n",
       " 261: 'although',\n",
       " 262: 'especially',\n",
       " 263: 'our',\n",
       " 264: 'believe',\n",
       " 265: 'course',\n",
       " 266: 'comes',\n",
       " 267: 'looking',\n",
       " 268: 'screen',\n",
       " 269: 'trying',\n",
       " 270: 'set',\n",
       " 271: 'goes',\n",
       " 272: 'looks',\n",
       " 273: 'place',\n",
       " 274: 'book',\n",
       " 275: 'different',\n",
       " 276: 'put',\n",
       " 277: 'ending',\n",
       " 278: 'money',\n",
       " 279: 'maybe',\n",
       " 280: 'once',\n",
       " 281: 'sense',\n",
       " 282: 'reason',\n",
       " 283: 'true',\n",
       " 284: 'actor',\n",
       " 285: 'everything',\n",
       " 286: \"wasn't\",\n",
       " 287: 'shows',\n",
       " 288: 'dvd',\n",
       " 289: 'three',\n",
       " 290: 'worth',\n",
       " 291: 'year',\n",
       " 292: 'job',\n",
       " 293: 'main',\n",
       " 294: 'someone',\n",
       " 295: 'together',\n",
       " 296: 'watched',\n",
       " 297: 'play',\n",
       " 298: 'american',\n",
       " 299: 'plays',\n",
       " 300: '1',\n",
       " 301: 'said',\n",
       " 302: 'effects',\n",
       " 303: 'later',\n",
       " 304: 'takes',\n",
       " 305: 'instead',\n",
       " 306: 'seem',\n",
       " 307: 'beautiful',\n",
       " 308: 'john',\n",
       " 309: 'himself',\n",
       " 310: 'version',\n",
       " 311: 'audience',\n",
       " 312: 'high',\n",
       " 313: 'house',\n",
       " 314: 'night',\n",
       " 315: 'during',\n",
       " 316: 'everyone',\n",
       " 317: 'left',\n",
       " 318: 'special',\n",
       " 319: 'seeing',\n",
       " 320: 'half',\n",
       " 321: 'excellent',\n",
       " 322: 'wife',\n",
       " 323: 'star',\n",
       " 324: 'shot',\n",
       " 325: 'war',\n",
       " 326: 'idea',\n",
       " 327: 'nice',\n",
       " 328: 'black',\n",
       " 329: 'less',\n",
       " 330: 'mind',\n",
       " 331: 'simply',\n",
       " 332: 'read',\n",
       " 333: 'second',\n",
       " 334: 'else',\n",
       " 335: \"you're\",\n",
       " 336: 'father',\n",
       " 337: 'fan',\n",
       " 338: 'poor',\n",
       " 339: 'help',\n",
       " 340: 'completely',\n",
       " 341: 'death',\n",
       " 342: '3',\n",
       " 343: 'used',\n",
       " 344: 'home',\n",
       " 345: 'either',\n",
       " 346: 'short',\n",
       " 347: 'line',\n",
       " 348: 'given',\n",
       " 349: 'men',\n",
       " 350: 'top',\n",
       " 351: 'dead',\n",
       " 352: 'budget',\n",
       " 353: 'try',\n",
       " 354: 'performances',\n",
       " 355: 'wrong',\n",
       " 356: 'classic',\n",
       " 357: 'boring',\n",
       " 358: 'enjoy',\n",
       " 359: 'need',\n",
       " 360: 'rest',\n",
       " 361: 'use',\n",
       " 362: 'kids',\n",
       " 363: 'hollywood',\n",
       " 364: 'low',\n",
       " 365: 'production',\n",
       " 366: 'until',\n",
       " 367: 'along',\n",
       " 368: 'full',\n",
       " 369: 'friends',\n",
       " 370: 'camera',\n",
       " 371: 'truly',\n",
       " 372: 'women',\n",
       " 373: 'awful',\n",
       " 374: 'video',\n",
       " 375: 'next',\n",
       " 376: 'tell',\n",
       " 377: 'remember',\n",
       " 378: 'couple',\n",
       " 379: 'stupid',\n",
       " 380: 'start',\n",
       " 381: 'stars',\n",
       " 382: 'perhaps',\n",
       " 383: 'sex',\n",
       " 384: 'mean',\n",
       " 385: 'came',\n",
       " 386: 'recommend',\n",
       " 387: 'let',\n",
       " 388: 'moments',\n",
       " 389: 'wonderful',\n",
       " 390: 'episode',\n",
       " 391: 'understand',\n",
       " 392: 'small',\n",
       " 393: 'face',\n",
       " 394: 'terrible',\n",
       " 395: 'playing',\n",
       " 396: 'school',\n",
       " 397: 'getting',\n",
       " 398: 'written',\n",
       " 399: 'doing',\n",
       " 400: 'often',\n",
       " 401: 'keep',\n",
       " 402: 'early',\n",
       " 403: 'name',\n",
       " 404: 'perfect',\n",
       " 405: 'style',\n",
       " 406: 'human',\n",
       " 407: 'definitely',\n",
       " 408: 'gives',\n",
       " 409: 'others',\n",
       " 410: 'itself',\n",
       " 411: 'lines',\n",
       " 412: 'live',\n",
       " 413: 'become',\n",
       " 414: 'dialogue',\n",
       " 415: 'person',\n",
       " 416: 'lost',\n",
       " 417: 'finally',\n",
       " 418: 'piece',\n",
       " 419: 'head',\n",
       " 420: 'case',\n",
       " 421: 'felt',\n",
       " 422: 'yes',\n",
       " 423: 'liked',\n",
       " 424: 'supposed',\n",
       " 425: 'title',\n",
       " 426: \"couldn't\",\n",
       " 427: 'absolutely',\n",
       " 428: 'white',\n",
       " 429: 'against',\n",
       " 430: 'boy',\n",
       " 431: 'picture',\n",
       " 432: 'sort',\n",
       " 433: 'worse',\n",
       " 434: 'certainly',\n",
       " 435: 'went',\n",
       " 436: 'entire',\n",
       " 437: 'waste',\n",
       " 438: 'cinema',\n",
       " 439: 'problem',\n",
       " 440: 'hope',\n",
       " 441: 'entertaining',\n",
       " 442: \"she's\",\n",
       " 443: 'mr',\n",
       " 444: 'overall',\n",
       " 445: 'evil',\n",
       " 446: 'called',\n",
       " 447: 'loved',\n",
       " 448: 'based',\n",
       " 449: 'oh',\n",
       " 450: 'several',\n",
       " 451: 'fans',\n",
       " 452: 'mother',\n",
       " 453: 'drama',\n",
       " 454: 'beginning',\n",
       " 455: 'killer',\n",
       " 456: 'lives',\n",
       " 457: '5',\n",
       " 458: 'direction',\n",
       " 459: 'care',\n",
       " 460: 'already',\n",
       " 461: 'becomes',\n",
       " 462: 'laugh',\n",
       " 463: 'example',\n",
       " 464: 'friend',\n",
       " 465: 'dark',\n",
       " 466: 'despite',\n",
       " 467: 'under',\n",
       " 468: 'seemed',\n",
       " 469: 'throughout',\n",
       " 470: '4',\n",
       " 471: 'turn',\n",
       " 472: 'unfortunately',\n",
       " 473: 'wanted',\n",
       " 474: \"i'd\",\n",
       " 475: '\\x96',\n",
       " 476: 'children',\n",
       " 477: 'final',\n",
       " 478: 'fine',\n",
       " 479: 'history',\n",
       " 480: 'amazing',\n",
       " 481: 'sound',\n",
       " 482: 'guess',\n",
       " 483: 'heart',\n",
       " 484: 'totally',\n",
       " 485: 'lead',\n",
       " 486: 'humor',\n",
       " 487: 'writing',\n",
       " 488: 'michael',\n",
       " 489: 'quality',\n",
       " 490: \"you'll\",\n",
       " 491: 'close',\n",
       " 492: 'son',\n",
       " 493: 'guys',\n",
       " 494: 'wants',\n",
       " 495: 'works',\n",
       " 496: 'behind',\n",
       " 497: 'tries',\n",
       " 498: 'art',\n",
       " 499: 'side',\n",
       " 500: 'game',\n",
       " 501: 'past',\n",
       " 502: 'able',\n",
       " 503: 'b',\n",
       " 504: 'days',\n",
       " 505: 'turns',\n",
       " 506: 'child',\n",
       " 507: \"they're\",\n",
       " 508: 'hand',\n",
       " 509: 'flick',\n",
       " 510: 'enjoyed',\n",
       " 511: 'act',\n",
       " 512: 'genre',\n",
       " 513: 'town',\n",
       " 514: 'favorite',\n",
       " 515: 'soon',\n",
       " 516: 'kill',\n",
       " 517: 'starts',\n",
       " 518: 'sometimes',\n",
       " 519: 'car',\n",
       " 520: 'gave',\n",
       " 521: 'run',\n",
       " 522: 'late',\n",
       " 523: 'eyes',\n",
       " 524: 'actress',\n",
       " 525: 'etc',\n",
       " 526: 'directed',\n",
       " 527: 'horrible',\n",
       " 528: \"won't\",\n",
       " 529: 'viewer',\n",
       " 530: 'brilliant',\n",
       " 531: 'parts',\n",
       " 532: 'self',\n",
       " 533: 'themselves',\n",
       " 534: 'hour',\n",
       " 535: 'expect',\n",
       " 536: 'thinking',\n",
       " 537: 'stories',\n",
       " 538: 'stuff',\n",
       " 539: 'girls',\n",
       " 540: 'obviously',\n",
       " 541: 'blood',\n",
       " 542: 'decent',\n",
       " 543: 'city',\n",
       " 544: 'voice',\n",
       " 545: 'highly',\n",
       " 546: 'myself',\n",
       " 547: 'feeling',\n",
       " 548: 'fight',\n",
       " 549: 'except',\n",
       " 550: 'slow',\n",
       " 551: 'matter',\n",
       " 552: 'type',\n",
       " 553: 'anyway',\n",
       " 554: 'kid',\n",
       " 555: 'roles',\n",
       " 556: 'killed',\n",
       " 557: 'heard',\n",
       " 558: 'god',\n",
       " 559: 'age',\n",
       " 560: 'says',\n",
       " 561: 'moment',\n",
       " 562: 'took',\n",
       " 563: 'leave',\n",
       " 564: 'writer',\n",
       " 565: 'strong',\n",
       " 566: 'cannot',\n",
       " 567: 'violence',\n",
       " 568: 'police',\n",
       " 569: 'hit',\n",
       " 570: 'stop',\n",
       " 571: 'happens',\n",
       " 572: 'particularly',\n",
       " 573: 'known',\n",
       " 574: 'involved',\n",
       " 575: 'happened',\n",
       " 576: 'extremely',\n",
       " 577: 'daughter',\n",
       " 578: 'obvious',\n",
       " 579: 'told',\n",
       " 580: 'chance',\n",
       " 581: 'living',\n",
       " 582: 'coming',\n",
       " 583: 'lack',\n",
       " 584: 'alone',\n",
       " 585: 'experience',\n",
       " 586: \"wouldn't\",\n",
       " 587: 'including',\n",
       " 588: 'murder',\n",
       " 589: 'attempt',\n",
       " 590: 's',\n",
       " 591: 'please',\n",
       " 592: 'james',\n",
       " 593: 'happen',\n",
       " 594: 'wonder',\n",
       " 595: 'crap',\n",
       " 596: 'ago',\n",
       " 597: 'brother',\n",
       " 598: \"film's\",\n",
       " 599: 'gore',\n",
       " 600: 'none',\n",
       " 601: 'complete',\n",
       " 602: 'interest',\n",
       " 603: 'score',\n",
       " 604: 'group',\n",
       " 605: 'cut',\n",
       " 606: 'simple',\n",
       " 607: 'save',\n",
       " 608: 'ok',\n",
       " 609: 'hell',\n",
       " 610: 'looked',\n",
       " 611: 'career',\n",
       " 612: 'number',\n",
       " 613: 'song',\n",
       " 614: 'possible',\n",
       " 615: 'seriously',\n",
       " 616: 'annoying',\n",
       " 617: 'shown',\n",
       " 618: 'exactly',\n",
       " 619: 'sad',\n",
       " 620: 'running',\n",
       " 621: 'musical',\n",
       " 622: 'serious',\n",
       " 623: 'taken',\n",
       " 624: 'yourself',\n",
       " 625: 'whose',\n",
       " 626: 'released',\n",
       " 627: 'cinematography',\n",
       " 628: 'david',\n",
       " 629: 'scary',\n",
       " 630: 'ends',\n",
       " 631: 'english',\n",
       " 632: 'hero',\n",
       " 633: 'usually',\n",
       " 634: 'hours',\n",
       " 635: 'reality',\n",
       " 636: 'opening',\n",
       " 637: \"i'll\",\n",
       " 638: 'across',\n",
       " 639: 'today',\n",
       " 640: 'jokes',\n",
       " 641: 'light',\n",
       " 642: 'hilarious',\n",
       " 643: 'somewhat',\n",
       " 644: 'usual',\n",
       " 645: 'started',\n",
       " 646: 'cool',\n",
       " 647: 'ridiculous',\n",
       " 648: 'body',\n",
       " 649: 'relationship',\n",
       " 650: 'view',\n",
       " 651: 'level',\n",
       " 652: 'opinion',\n",
       " 653: 'change',\n",
       " 654: 'happy',\n",
       " 655: 'middle',\n",
       " 656: 'taking',\n",
       " 657: 'wish',\n",
       " 658: 'husband',\n",
       " 659: 'finds',\n",
       " 660: 'saying',\n",
       " 661: 'order',\n",
       " 662: 'talking',\n",
       " 663: 'ones',\n",
       " 664: 'documentary',\n",
       " 665: 'shots',\n",
       " 666: 'huge',\n",
       " 667: 'novel',\n",
       " 668: 'female',\n",
       " 669: 'mostly',\n",
       " 670: 'robert',\n",
       " 671: 'power',\n",
       " 672: 'episodes',\n",
       " 673: 'room',\n",
       " 674: 'important',\n",
       " 675: 'rating',\n",
       " 676: 'talent',\n",
       " 677: 'five',\n",
       " 678: 'major',\n",
       " 679: 'turned',\n",
       " 680: 'strange',\n",
       " 681: 'word',\n",
       " 682: 'modern',\n",
       " 683: 'call',\n",
       " 684: 'apparently',\n",
       " 685: 'disappointed',\n",
       " 686: 'single',\n",
       " 687: 'events',\n",
       " 688: 'due',\n",
       " 689: 'four',\n",
       " 690: 'songs',\n",
       " 691: 'basically',\n",
       " 692: 'attention',\n",
       " 693: '7',\n",
       " 694: 'knows',\n",
       " 695: 'clearly',\n",
       " 696: 'supporting',\n",
       " 697: 'knew',\n",
       " 698: 'british',\n",
       " 699: 'television',\n",
       " 700: 'comic',\n",
       " 701: 'non',\n",
       " 702: 'fast',\n",
       " 703: 'earth',\n",
       " 704: 'country',\n",
       " 705: 'future',\n",
       " 706: 'cheap',\n",
       " 707: 'class',\n",
       " 708: 'thriller',\n",
       " 709: '8',\n",
       " 710: 'silly',\n",
       " 711: 'king',\n",
       " 712: 'problems',\n",
       " 713: \"aren't\",\n",
       " 714: 'easily',\n",
       " 715: 'words',\n",
       " 716: 'tells',\n",
       " 717: 'miss',\n",
       " 718: 'jack',\n",
       " 719: 'local',\n",
       " 720: 'sequence',\n",
       " 721: 'bring',\n",
       " 722: 'entertainment',\n",
       " 723: 'paul',\n",
       " 724: 'beyond',\n",
       " 725: 'upon',\n",
       " 726: 'whether',\n",
       " 727: 'predictable',\n",
       " 728: 'moving',\n",
       " 729: 'similar',\n",
       " 730: 'straight',\n",
       " 731: 'romantic',\n",
       " 732: 'sets',\n",
       " 733: 'review',\n",
       " 734: 'falls',\n",
       " 735: 'oscar',\n",
       " 736: 'mystery',\n",
       " 737: 'enjoyable',\n",
       " 738: 'needs',\n",
       " 739: 'appears',\n",
       " 740: 'talk',\n",
       " 741: 'rock',\n",
       " 742: 'george',\n",
       " 743: 'giving',\n",
       " 744: 'eye',\n",
       " 745: 'richard',\n",
       " 746: 'within',\n",
       " 747: 'ten',\n",
       " 748: 'animation',\n",
       " 749: 'message',\n",
       " 750: 'theater',\n",
       " 751: 'near',\n",
       " 752: 'above',\n",
       " 753: 'dull',\n",
       " 754: 'nearly',\n",
       " 755: 'sequel',\n",
       " 756: 'theme',\n",
       " 757: 'points',\n",
       " 758: \"'\",\n",
       " 759: 'stand',\n",
       " 760: 'mention',\n",
       " 761: 'lady',\n",
       " 762: 'bunch',\n",
       " 763: 'add',\n",
       " 764: 'feels',\n",
       " 765: 'herself',\n",
       " 766: 'release',\n",
       " 767: 'red',\n",
       " 768: 'team',\n",
       " 769: 'storyline',\n",
       " 770: 'surprised',\n",
       " 771: 'ways',\n",
       " 772: 'using',\n",
       " 773: 'named',\n",
       " 774: \"haven't\",\n",
       " 775: 'lots',\n",
       " 776: 'easy',\n",
       " 777: 'fantastic',\n",
       " 778: 'begins',\n",
       " 779: 'actual',\n",
       " 780: 'working',\n",
       " 781: 'effort',\n",
       " 782: 'york',\n",
       " 783: 'die',\n",
       " 784: 'hate',\n",
       " 785: 'french',\n",
       " 786: 'minute',\n",
       " 787: 'tale',\n",
       " 788: 'clear',\n",
       " 789: 'stay',\n",
       " 790: '9',\n",
       " 791: 'elements',\n",
       " 792: 'feature',\n",
       " 793: 'among',\n",
       " 794: 'follow',\n",
       " 795: 'comments',\n",
       " 796: 're',\n",
       " 797: 'viewers',\n",
       " 798: 'avoid',\n",
       " 799: 'sister',\n",
       " 800: 'showing',\n",
       " 801: 'typical',\n",
       " 802: 'editing',\n",
       " 803: \"what's\",\n",
       " 804: 'famous',\n",
       " 805: 'tried',\n",
       " 806: 'sorry',\n",
       " 807: 'dialog',\n",
       " 808: 'check',\n",
       " 809: 'fall',\n",
       " 810: 'period',\n",
       " 811: 'season',\n",
       " 812: 'form',\n",
       " 813: 'certain',\n",
       " 814: 'filmed',\n",
       " 815: 'weak',\n",
       " 816: 'soundtrack',\n",
       " 817: 'means',\n",
       " 818: 'buy',\n",
       " 819: 'material',\n",
       " 820: 'somehow',\n",
       " 821: 'realistic',\n",
       " 822: 'figure',\n",
       " 823: 'crime',\n",
       " 824: 'doubt',\n",
       " 825: 'gone',\n",
       " 826: 'peter',\n",
       " 827: 'tom',\n",
       " 828: 'kept',\n",
       " 829: 'viewing',\n",
       " 830: 't',\n",
       " 831: 'general',\n",
       " 832: 'leads',\n",
       " 833: 'greatest',\n",
       " 834: 'space',\n",
       " 835: 'lame',\n",
       " 836: 'suspense',\n",
       " 837: 'dance',\n",
       " 838: 'imagine',\n",
       " 839: 'brought',\n",
       " 840: 'third',\n",
       " 841: 'atmosphere',\n",
       " 842: 'hear',\n",
       " 843: 'particular',\n",
       " 844: 'sequences',\n",
       " 845: 'whatever',\n",
       " 846: 'parents',\n",
       " 847: 'move',\n",
       " 848: 'lee',\n",
       " 849: 'indeed',\n",
       " 850: 'learn',\n",
       " 851: 'rent',\n",
       " 852: 'de',\n",
       " 853: 'eventually',\n",
       " 854: 'note',\n",
       " 855: 'deal',\n",
       " 856: 'average',\n",
       " 857: 'reviews',\n",
       " 858: 'wait',\n",
       " 859: 'forget',\n",
       " 860: 'japanese',\n",
       " 861: 'sexual',\n",
       " 862: 'poorly',\n",
       " 863: 'premise',\n",
       " 864: 'okay',\n",
       " 865: 'zombie',\n",
       " 866: 'surprise',\n",
       " 867: 'believable',\n",
       " 868: 'stage',\n",
       " 869: 'possibly',\n",
       " 870: 'sit',\n",
       " 871: \"who's\",\n",
       " 872: 'decided',\n",
       " 873: 'expected',\n",
       " 874: \"you've\",\n",
       " 875: 'subject',\n",
       " 876: 'nature',\n",
       " 877: 'became',\n",
       " 878: 'difficult',\n",
       " 879: 'free',\n",
       " 880: 'killing',\n",
       " 881: 'screenplay',\n",
       " 882: 'truth',\n",
       " 883: 'romance',\n",
       " 884: 'dr',\n",
       " 885: 'nor',\n",
       " 886: 'reading',\n",
       " 887: 'needed',\n",
       " 888: 'question',\n",
       " 889: 'leaves',\n",
       " 890: 'street',\n",
       " 891: '20',\n",
       " 892: 'meets',\n",
       " 893: 'hot',\n",
       " 894: 'unless',\n",
       " 895: 'begin',\n",
       " 896: 'baby',\n",
       " 897: 'superb',\n",
       " 898: 'credits',\n",
       " 899: 'imdb',\n",
       " 900: 'otherwise',\n",
       " 901: 'write',\n",
       " 902: 'shame',\n",
       " 903: \"let's\",\n",
       " 904: 'situation',\n",
       " 905: 'dramatic',\n",
       " 906: 'memorable',\n",
       " 907: 'directors',\n",
       " 908: 'earlier',\n",
       " 909: 'meet',\n",
       " 910: 'disney',\n",
       " 911: 'open',\n",
       " 912: 'dog',\n",
       " 913: 'badly',\n",
       " 914: 'joe',\n",
       " 915: 'male',\n",
       " 916: 'weird',\n",
       " 917: 'acted',\n",
       " 918: 'forced',\n",
       " 919: 'laughs',\n",
       " 920: 'sci',\n",
       " 921: 'emotional',\n",
       " 922: 'older',\n",
       " 923: 'realize',\n",
       " 924: 'fi',\n",
       " 925: 'dream',\n",
       " 926: 'society',\n",
       " 927: 'writers',\n",
       " 928: 'interested',\n",
       " 929: 'footage',\n",
       " 930: 'forward',\n",
       " 931: 'comment',\n",
       " 932: 'crazy',\n",
       " 933: 'deep',\n",
       " 934: 'sounds',\n",
       " 935: 'plus',\n",
       " 936: 'beauty',\n",
       " 937: 'whom',\n",
       " 938: 'america',\n",
       " 939: 'fantasy',\n",
       " 940: 'directing',\n",
       " 941: 'keeps',\n",
       " 942: 'ask',\n",
       " 943: 'development',\n",
       " 944: 'features',\n",
       " 945: 'air',\n",
       " 946: 'quickly',\n",
       " 947: 'mess',\n",
       " 948: 'creepy',\n",
       " 949: 'towards',\n",
       " 950: 'perfectly',\n",
       " 951: 'mark',\n",
       " 952: 'worked',\n",
       " 953: 'box',\n",
       " 954: 'cheesy',\n",
       " 955: 'unique',\n",
       " 956: 'setting',\n",
       " 957: 'hands',\n",
       " 958: 'plenty',\n",
       " 959: 'result',\n",
       " 960: 'previous',\n",
       " 961: 'brings',\n",
       " 962: 'effect',\n",
       " 963: 'e',\n",
       " 964: 'total',\n",
       " 965: 'personal',\n",
       " 966: 'incredibly',\n",
       " 967: 'rate',\n",
       " 968: 'fire',\n",
       " 969: 'monster',\n",
       " 970: 'business',\n",
       " 971: 'leading',\n",
       " 972: 'apart',\n",
       " 973: 'casting',\n",
       " 974: 'admit',\n",
       " 975: 'joke',\n",
       " 976: 'powerful',\n",
       " 977: 'appear',\n",
       " 978: 'background',\n",
       " 979: 'telling',\n",
       " 980: 'girlfriend',\n",
       " 981: 'meant',\n",
       " 982: 'christmas',\n",
       " 983: 'hardly',\n",
       " 984: 'present',\n",
       " 985: 'battle',\n",
       " 986: 'potential',\n",
       " 987: 'create',\n",
       " 988: 'bill',\n",
       " 989: 'break',\n",
       " 990: 'pay',\n",
       " 991: 'masterpiece',\n",
       " 992: 'gay',\n",
       " 993: 'political',\n",
       " 994: 'return',\n",
       " 995: 'dumb',\n",
       " 996: 'fails',\n",
       " 997: 'fighting',\n",
       " 998: 'various',\n",
       " 999: 'era',\n",
       " 1000: 'portrayed',\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "x_train :  25000  x_test :  25000\n",
      "y_train :  25000  y_test :  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data: \")\n",
    "print(\"x_train : \",len(train[0]),\" x_test : \",len(train[1]))\n",
    "print(\"y_train : \",len(test[0]),\" y_test : \",len(test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((x_train, x_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: \n",
      "998\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique words: \")\n",
    "print(len(np.unique(np.hstack(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long short-term memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80 \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LSTM Model And Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.4636 - acc: 0.7804 - val_loss: 0.4571 - val_acc: 0.7823\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 90s 4ms/step - loss: 0.2968 - acc: 0.8799 - val_loss: 0.4114 - val_acc: 0.8304\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 90s 4ms/step - loss: 0.2186 - acc: 0.9142 - val_loss: 0.4305 - val_acc: 0.8260\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.1570 - acc: 0.9390 - val_loss: 0.4570 - val_acc: 0.8286\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 90s 4ms/step - loss: 0.1047 - acc: 0.9632 - val_loss: 0.5843 - val_acc: 0.8280\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.0701 - acc: 0.9755 - val_loss: 0.7365 - val_acc: 0.8169\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.0621 - acc: 0.9788 - val_loss: 0.7565 - val_acc: 0.8155\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.0456 - acc: 0.9851 - val_loss: 0.7610 - val_acc: 0.8226\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.0305 - acc: 0.9894 - val_loss: 0.9996 - val_acc: 0.8153\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.0256 - acc: 0.9918 - val_loss: 1.0215 - val_acc: 0.8174\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.0198 - acc: 0.9938 - val_loss: 0.9382 - val_acc: 0.8145\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.0153 - acc: 0.9951 - val_loss: 1.0535 - val_acc: 0.8139\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.0185 - acc: 0.9943 - val_loss: 1.0554 - val_acc: 0.8164\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.0070 - acc: 0.9978 - val_loss: 1.0905 - val_acc: 0.8092\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.0094 - acc: 0.9971 - val_loss: 1.0179 - val_acc: 0.8104\n",
      "25000/25000 [==============================] - 16s 649us/step\n",
      "Test score: 1.0178739750412107\n",
      "Test accuracy: 0.81036\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperarameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune units in LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 64 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model2 with LSTM 64 units...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.4764 - acc: 0.7712 - val_loss: 0.3977 - val_acc: 0.8219\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.3038 - acc: 0.8770 - val_loss: 0.3836 - val_acc: 0.8396\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.2245 - acc: 0.9119 - val_loss: 0.4235 - val_acc: 0.8312\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 90s 4ms/step - loss: 0.1660 - acc: 0.9381 - val_loss: 0.4889 - val_acc: 0.8295\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.1287 - acc: 0.9522 - val_loss: 0.5380 - val_acc: 0.8224\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.1038 - acc: 0.9633 - val_loss: 0.6350 - val_acc: 0.8203\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.0731 - acc: 0.9756 - val_loss: 0.6487 - val_acc: 0.8219\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.0617 - acc: 0.9790 - val_loss: 0.8456 - val_acc: 0.8092\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.0504 - acc: 0.9832 - val_loss: 0.7266 - val_acc: 0.8138\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.0482 - acc: 0.9843 - val_loss: 0.7979 - val_acc: 0.8131\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.0363 - acc: 0.9883 - val_loss: 0.9006 - val_acc: 0.8140\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.0294 - acc: 0.9896 - val_loss: 0.9170 - val_acc: 0.8133\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.0250 - acc: 0.9923 - val_loss: 1.0455 - val_acc: 0.7970\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.0239 - acc: 0.9930 - val_loss: 1.0561 - val_acc: 0.8140\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.0193 - acc: 0.9934 - val_loss: 0.9271 - val_acc: 0.8089\n",
      "25000/25000 [==============================] - 17s 663us/step\n",
      "Test score: 0.9270928121328353\n",
      "Test accuracy: 0.80888\n"
     ]
    }
   ],
   "source": [
    "# units: Positive integer, dimensionality of the output space.\n",
    "\n",
    "print('Build model2 with LSTM 64 units...')\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(max_features, 64))\n",
    "model2.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model2.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model2.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- 256 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model3 with LSTM 256 units...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 95s 4ms/step - loss: 0.4759 - acc: 0.7695 - val_loss: 0.4238 - val_acc: 0.8168\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.2891 - acc: 0.8822 - val_loss: 0.3824 - val_acc: 0.8343\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.1881 - acc: 0.9286 - val_loss: 0.4451 - val_acc: 0.8303\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.1229 - acc: 0.9568 - val_loss: 0.5214 - val_acc: 0.8217\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.0789 - acc: 0.9717 - val_loss: 0.6120 - val_acc: 0.8268\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.0530 - acc: 0.9834 - val_loss: 0.7341 - val_acc: 0.8117\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.0319 - acc: 0.9890 - val_loss: 0.8329 - val_acc: 0.8157\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.0240 - acc: 0.9923 - val_loss: 0.8652 - val_acc: 0.8159\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.0161 - acc: 0.9949 - val_loss: 0.9776 - val_acc: 0.8159\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.0126 - acc: 0.9962 - val_loss: 0.9983 - val_acc: 0.8081\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.0127 - acc: 0.9956 - val_loss: 1.1114 - val_acc: 0.8136\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.0114 - acc: 0.9966 - val_loss: 1.0851 - val_acc: 0.8137\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.0088 - acc: 0.9970 - val_loss: 1.1823 - val_acc: 0.8103\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.0041 - acc: 0.9991 - val_loss: 1.1869 - val_acc: 0.8096\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.0057 - acc: 0.9983 - val_loss: 1.2638 - val_acc: 0.8077\n",
      "25000/25000 [==============================] - 16s 653us/step\n",
      "Test score: 1.2637602571652644\n",
      "Test accuracy: 0.80772\n"
     ]
    }
   ],
   "source": [
    "print('Build model3 with LSTM 256 units...')\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(max_features, 256))\n",
    "model3.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model3.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model3.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Tune batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model4 with batch_size = 64...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.4641 - acc: 0.7730 - val_loss: 0.3749 - val_acc: 0.8369\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.2983 - acc: 0.8778 - val_loss: 0.3726 - val_acc: 0.8385\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.2195 - acc: 0.9169 - val_loss: 0.4407 - val_acc: 0.8313\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.1560 - acc: 0.9433 - val_loss: 0.4640 - val_acc: 0.8198\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.1135 - acc: 0.9590 - val_loss: 0.6808 - val_acc: 0.8192\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0912 - acc: 0.9672 - val_loss: 0.5813 - val_acc: 0.8204\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0642 - acc: 0.9784 - val_loss: 0.7899 - val_acc: 0.8179\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0544 - acc: 0.9818 - val_loss: 0.8741 - val_acc: 0.8132\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0418 - acc: 0.9874 - val_loss: 0.8165 - val_acc: 0.8134\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0322 - acc: 0.9887 - val_loss: 0.8911 - val_acc: 0.8121\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0308 - acc: 0.9895 - val_loss: 0.9144 - val_acc: 0.8126\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0262 - acc: 0.9915 - val_loss: 0.9872 - val_acc: 0.8144\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0186 - acc: 0.9945 - val_loss: 0.9923 - val_acc: 0.8081\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0200 - acc: 0.9935 - val_loss: 0.9936 - val_acc: 0.8112\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0179 - acc: 0.9945 - val_loss: 0.9715 - val_acc: 0.8100\n",
      "25000/25000 [==============================] - 8s 324us/step\n",
      "Test score: 0.9715033755111694\n",
      "Test accuracy: 0.8099999999809265\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "print('Build model4 with batch_size = 64...')\n",
    "model4 = Sequential()\n",
    "model4.add(Embedding(max_features, 128))\n",
    "model4.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model4.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model4.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model4.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- batch size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model5 with batch_size = 256...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 12s 492us/step - loss: 0.5147 - acc: 0.7382 - val_loss: 0.3760 - val_acc: 0.8323\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 11s 456us/step - loss: 0.3077 - acc: 0.8766 - val_loss: 0.3856 - val_acc: 0.8269\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 11s 456us/step - loss: 0.2457 - acc: 0.9083 - val_loss: 0.4094 - val_acc: 0.8324\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 11s 455us/step - loss: 0.1976 - acc: 0.9254 - val_loss: 0.4367 - val_acc: 0.8281\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 11s 456us/step - loss: 0.1663 - acc: 0.9401 - val_loss: 0.4514 - val_acc: 0.8164\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 11s 459us/step - loss: 0.1445 - acc: 0.9476 - val_loss: 0.4907 - val_acc: 0.8193\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 12s 460us/step - loss: 0.1099 - acc: 0.9619 - val_loss: 0.5670 - val_acc: 0.8130\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 11s 458us/step - loss: 0.0911 - acc: 0.9700 - val_loss: 0.6642 - val_acc: 0.8143\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 12s 460us/step - loss: 0.0807 - acc: 0.9719 - val_loss: 0.6692 - val_acc: 0.8138\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 12s 461us/step - loss: 0.0725 - acc: 0.9766 - val_loss: 0.7077 - val_acc: 0.8108\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 11s 458us/step - loss: 0.0653 - acc: 0.9782 - val_loss: 0.7621 - val_acc: 0.8101\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 12s 461us/step - loss: 0.0601 - acc: 0.9798 - val_loss: 0.6589 - val_acc: 0.8036\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 11s 459us/step - loss: 0.0511 - acc: 0.9830 - val_loss: 0.7456 - val_acc: 0.7996\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 11s 458us/step - loss: 0.0369 - acc: 0.9884 - val_loss: 0.9301 - val_acc: 0.8049\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 11s 456us/step - loss: 0.0304 - acc: 0.9910 - val_loss: 0.9567 - val_acc: 0.8042\n",
      "25000/25000 [==============================] - 2s 85us/step\n",
      "Test score: 0.9567267325592042\n",
      "Test accuracy: 0.8042400000190735\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "print('Build model5 with batch_size = 256...')\n",
    "model5 = Sequential()\n",
    "model5.add(Embedding(max_features, 128))\n",
    "model5.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model5.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model5.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model5.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model5.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Tune optimizer\n",
    "- use Stochastic gradient descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model6 with optimizer = SGD...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.6932 - acc: 0.5021 - val_loss: 0.6932 - val_acc: 0.5003\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.6930 - acc: 0.5043 - val_loss: 0.6928 - val_acc: 0.5061\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.6927 - acc: 0.5140 - val_loss: 0.6925 - val_acc: 0.5208\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.6925 - acc: 0.5202 - val_loss: 0.6924 - val_acc: 0.5088\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.6922 - acc: 0.5310 - val_loss: 0.6920 - val_acc: 0.5440\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.6919 - acc: 0.5379 - val_loss: 0.6917 - val_acc: 0.5470\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.6915 - acc: 0.5448 - val_loss: 0.6915 - val_acc: 0.5184\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.6912 - acc: 0.5458 - val_loss: 0.6910 - val_acc: 0.5583\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.6908 - acc: 0.5558 - val_loss: 0.6906 - val_acc: 0.5772\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.6902 - acc: 0.5543 - val_loss: 0.6902 - val_acc: 0.5572\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.6897 - acc: 0.5685 - val_loss: 0.6896 - val_acc: 0.5555\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.6890 - acc: 0.5637 - val_loss: 0.6888 - val_acc: 0.5860\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.6882 - acc: 0.5794 - val_loss: 0.6881 - val_acc: 0.5625\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.6872 - acc: 0.5862 - val_loss: 0.6869 - val_acc: 0.5838\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.6858 - acc: 0.5864 - val_loss: 0.6854 - val_acc: 0.5940\n",
      "25000/25000 [==============================] - 17s 671us/step\n",
      "Test score: 0.6854370599746704\n",
      "Test accuracy: 0.59404\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "print('Build model6 with optimizer = SGD...')\n",
    "model6 = Sequential()\n",
    "model6.add(Embedding(max_features, 128))\n",
    "model6.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model6.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model6.compile(loss='binary_crossentropy',\n",
    "              optimizer='SGD',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model6.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model6.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- use RMSProp optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model6 with optimizer = RMSprop...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.4527 - acc: 0.7935 - val_loss: 0.3765 - val_acc: 0.8378\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.3291 - acc: 0.8623 - val_loss: 0.4226 - val_acc: 0.7984\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.2859 - acc: 0.8832 - val_loss: 0.3625 - val_acc: 0.8471\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.2542 - acc: 0.8969 - val_loss: 0.3413 - val_acc: 0.8538\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.2244 - acc: 0.9101 - val_loss: 0.3457 - val_acc: 0.8524\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.2038 - acc: 0.9196 - val_loss: 0.3794 - val_acc: 0.8530\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.1820 - acc: 0.9312 - val_loss: 0.3887 - val_acc: 0.8422\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.1632 - acc: 0.9380 - val_loss: 0.3918 - val_acc: 0.8469\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.1492 - acc: 0.9444 - val_loss: 0.4118 - val_acc: 0.8471\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.1306 - acc: 0.9528 - val_loss: 0.4288 - val_acc: 0.8440\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.1142 - acc: 0.9586 - val_loss: 0.4414 - val_acc: 0.8342\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.0980 - acc: 0.9660 - val_loss: 0.4770 - val_acc: 0.8408\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.0859 - acc: 0.9687 - val_loss: 0.5342 - val_acc: 0.8310\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.0742 - acc: 0.9740 - val_loss: 0.5508 - val_acc: 0.8335\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.0610 - acc: 0.9789 - val_loss: 0.5831 - val_acc: 0.8269\n",
      "25000/25000 [==============================] - 17s 669us/step\n",
      "Test score: 0.5830579057455063\n",
      "Test accuracy: 0.82688\n"
     ]
    }
   ],
   "source": [
    "print('Build model7 with optimizer = RMSprop...')\n",
    "model7 = Sequential()\n",
    "model7.add(Embedding(max_features, 128))\n",
    "model7.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model7.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model7.compile(loss='binary_crossentropy',\n",
    "              optimizer='RMSprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model7.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model7.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tune Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- optimizer = RMSprop + add hidden layer in lstm + batch size = 128' + epoch = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model8 with optimizer = RMSprop + add hidden layer in lstm + batch size = 128\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/30\n",
      "25000/25000 [==============================] - 48s 2ms/step - loss: 0.4814 - acc: 0.7674 - val_loss: 0.4053 - val_acc: 0.8237\n",
      "Epoch 2/30\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.3238 - acc: 0.8654 - val_loss: 0.4111 - val_acc: 0.8243\n",
      "Epoch 3/30\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.2709 - acc: 0.8917 - val_loss: 0.3702 - val_acc: 0.8380\n",
      "Epoch 4/30\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.2330 - acc: 0.9091 - val_loss: 0.4293 - val_acc: 0.8372\n",
      "Epoch 5/30\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.2097 - acc: 0.9211 - val_loss: 0.4741 - val_acc: 0.7999\n",
      "Epoch 6/30\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.1827 - acc: 0.9322 - val_loss: 0.4247 - val_acc: 0.8252\n",
      "Epoch 7/30\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.1594 - acc: 0.9419 - val_loss: 0.4097 - val_acc: 0.8271\n",
      "Epoch 8/30\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.1402 - acc: 0.9496 - val_loss: 0.5378 - val_acc: 0.8273\n",
      "Epoch 9/30\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.1233 - acc: 0.9554 - val_loss: 0.4754 - val_acc: 0.8211\n",
      "Epoch 10/30\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.1041 - acc: 0.9627 - val_loss: 0.5812 - val_acc: 0.8244\n",
      "Epoch 11/30\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0898 - acc: 0.9681 - val_loss: 0.6804 - val_acc: 0.8176\n",
      "Epoch 12/30\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0757 - acc: 0.9732 - val_loss: 0.5820 - val_acc: 0.8214\n",
      "Epoch 13/30\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0652 - acc: 0.9775 - val_loss: 0.7488 - val_acc: 0.7964\n",
      "Epoch 14/30\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0545 - acc: 0.9813 - val_loss: 0.7289 - val_acc: 0.8132\n",
      "Epoch 15/30\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0427 - acc: 0.9849 - val_loss: 0.8413 - val_acc: 0.8049\n",
      "Epoch 16/30\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0388 - acc: 0.9864 - val_loss: 0.8472 - val_acc: 0.8140\n",
      "Epoch 17/30\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.0298 - acc: 0.9898 - val_loss: 0.9112 - val_acc: 0.8073\n",
      "Epoch 18/30\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0253 - acc: 0.9912 - val_loss: 0.9124 - val_acc: 0.8046\n",
      "Epoch 19/30\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.0222 - acc: 0.9922 - val_loss: 1.0144 - val_acc: 0.8042\n",
      "Epoch 20/30\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.0170 - acc: 0.9947 - val_loss: 1.0127 - val_acc: 0.8060\n",
      "Epoch 21/30\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.0133 - acc: 0.9954 - val_loss: 1.1026 - val_acc: 0.8059\n",
      "Epoch 22/30\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.0112 - acc: 0.9963 - val_loss: 1.1562 - val_acc: 0.8059\n",
      "Epoch 23/30\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0092 - acc: 0.9968 - val_loss: 1.2988 - val_acc: 0.7988\n",
      "Epoch 24/30\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.0096 - acc: 0.9970 - val_loss: 1.3055 - val_acc: 0.8022\n",
      "Epoch 25/30\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0096 - acc: 0.9967 - val_loss: 1.1673 - val_acc: 0.8019\n",
      "Epoch 26/30\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0068 - acc: 0.9981 - val_loss: 1.2197 - val_acc: 0.7990\n",
      "Epoch 27/30\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0066 - acc: 0.9978 - val_loss: 1.1937 - val_acc: 0.7993\n",
      "Epoch 28/30\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0047 - acc: 0.9986 - val_loss: 1.4634 - val_acc: 0.8048\n",
      "Epoch 29/30\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0049 - acc: 0.9987 - val_loss: 1.4681 - val_acc: 0.7971\n",
      "Epoch 30/30\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0047 - acc: 0.9984 - val_loss: 1.3992 - val_acc: 0.8036\n",
      "25000/25000 [==============================] - 8s 333us/step\n",
      "Test score: 1.3992165909576415\n",
      "Test accuracy: 0.8035999999809265\n"
     ]
    }
   ],
   "source": [
    "- optimizer = RMSprop + add hidden layer in lstm + batch size = 128' + epoch = 30batch_size = 128\n",
    "print('Build model8 with optimizer = RMSprop + add hidden layer in lstm + batch size = 128')\n",
    "model8 = Sequential()\n",
    "model8.add(Embedding(max_features, 128))\n",
    "model8.add(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "model8.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model8.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model8.compile(loss='binary_crossentropy',\n",
    "              optimizer='RMSprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model8.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=30,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model8.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, None, 128)         131584    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,823,297\n",
      "Trainable params: 2,823,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "print('Build model8 with optimizer = RMSprop + add hidden layer in lstm + batch size = 128')\n",
    "model8 = Sequential()\n",
    "model8.add(Embedding(max_features, 128))\n",
    "model8.add(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "model8.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model8.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model8.compile(loss='binary_crossentropy',\n",
    "              optimizer='RMSprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model8.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=30,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model8.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)model8.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model9 with optimizer = Adam + add hidden layer in lstm + batch size = 32...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.4688 - acc: 0.7707 - val_loss: 0.3907 - val_acc: 0.8234\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.3010 - acc: 0.8796 - val_loss: 0.3931 - val_acc: 0.8295\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.2233 - acc: 0.9150 - val_loss: 0.4351 - val_acc: 0.8275\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.1670 - acc: 0.9397 - val_loss: 0.4549 - val_acc: 0.8253\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.1207 - acc: 0.9576 - val_loss: 0.5265 - val_acc: 0.8199\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0883 - acc: 0.9698 - val_loss: 0.5733 - val_acc: 0.8152\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0668 - acc: 0.9774 - val_loss: 0.7177 - val_acc: 0.8086\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0552 - acc: 0.9802 - val_loss: 0.8109 - val_acc: 0.8098\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.0416 - acc: 0.9870 - val_loss: 0.8691 - val_acc: 0.8093\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0363 - acc: 0.9887 - val_loss: 0.9016 - val_acc: 0.8086\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0259 - acc: 0.9918 - val_loss: 0.9278 - val_acc: 0.8022\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0240 - acc: 0.9924 - val_loss: 0.9858 - val_acc: 0.8062\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.0268 - acc: 0.9910 - val_loss: 0.8930 - val_acc: 0.8054\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.0208 - acc: 0.9928 - val_loss: 1.0109 - val_acc: 0.8032\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0204 - acc: 0.9933 - val_loss: 1.0763 - val_acc: 0.8054\n",
      "25000/25000 [==============================] - 8s 330us/step\n",
      "Test score: 1.0762920999908447\n",
      "Test accuracy: 0.8054000000381469\n"
     ]
    }
   ],
   "source": [
    "model8.summary()print('Build model9 with optimizer = Adam + add hidden layer in lstm + batch size = 32...')\n",
    "model9 = Sequential()\n",
    "model9.add(Embedding(max_features, 128))\n",
    "model9.add(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "model9.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model9.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model9.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model9.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model9.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, None, 128)         131584    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,823,297\n",
      "Trainable params: 2,823,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build model9 with optimizer = Adam + add hidden layer in lstm + batch size = 32...')\n",
    "model9 = Sequential()\n",
    "model9.add(Embedding(max_features, 128))\n",
    "model9.add(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "model9.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model9.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model9.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model9.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model9.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)model9.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- สำหรับ Model LSTM ที่ดีที่สุดน่าจะเป็น Model ที่ 7 ซึ่งมีค่า Test accuracy: 0.82688 มากที่สุดในทุกโมเดล และ จากค่า loss ใน epoch ล่าสุด คาดว่ายังสามารถเพิ่มจำนวน epoch เพื่อเพิ่มความแม่นยำ ให้กับโมเดลได้อีกด้วย\n",
    "- สำหรับ Model LSTM ที่เพิ่มจำนวน hidden layer เข้าไปพบว่าอาจทำให้เกิดการ overfit เกินไปกับตัว train data ทำให้ Test Accuracy ที่ได้ ไม่มากเท่าที่คิด หรืออาจเกิดจากการปรับค่า Parameter ได้ไม่ดีพอ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                               Model                              | Test Accuracy | Test Score         |\n",
    "|:----------------------------------------------------------------:|---------------|--------------------|\n",
    "|                            Base model                            |       0.81036 | 1.0178739750412107 |\n",
    "|                           LSTM 64 units                          |       0.80888 | 0.9270928121328353 |\n",
    "|                          LSTM 256 units                          |       0.80772 | 1.2637602571652644 |\n",
    "|                          batch size = 64                         |       0.80999 | 0.9715033755111694 |\n",
    "|                         batch size = 256                         |       0.80424 | 0.9567267325592042 |\n",
    "|                           SGD optimizer                          |       0.59404 | 0.6854370599746704 |\n",
    "|                         RMSProp optimizer                        |       0.82688 | 0.5830579057455063 |\n",
    "| 2 hidden layer in lstm + RMSprop + batch size = 128 + epoch = 30 |       0.80359 | 1.3992165909576415 |\n",
    "| 2 hidden layer in lstm + adam + batch size = 32 + epoch = 15     |       0.80540 | 1.0762920999908447 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base model Parameter\n",
    "1. batch size = 32\n",
    "2. epoch = 15\n",
    "3. LSTM 128 units 1 hidden layer\n",
    "4. adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 400, 50)           250000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 398, 250)          37750     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 350,751\n",
      "Trainable params: 350,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/8\n",
      "25000/25000 [==============================] - 87s 3ms/step - loss: 0.4027 - acc: 0.8010 - val_loss: 0.3173 - val_acc: 0.8617\n",
      "Epoch 2/8\n",
      "25000/25000 [==============================] - 86s 3ms/step - loss: 0.2280 - acc: 0.9099 - val_loss: 0.2778 - val_acc: 0.8835\n",
      "Epoch 3/8\n",
      "25000/25000 [==============================] - 86s 3ms/step - loss: 0.1627 - acc: 0.9374 - val_loss: 0.2691 - val_acc: 0.8910\n",
      "Epoch 4/8\n",
      "25000/25000 [==============================] - 86s 3ms/step - loss: 0.1114 - acc: 0.9590 - val_loss: 0.3591 - val_acc: 0.8806\n",
      "Epoch 5/8\n",
      "25000/25000 [==============================] - 86s 3ms/step - loss: 0.0791 - acc: 0.9719 - val_loss: 0.3894 - val_acc: 0.8819\n",
      "Epoch 6/8\n",
      "25000/25000 [==============================] - 86s 3ms/step - loss: 0.0575 - acc: 0.9790 - val_loss: 0.4305 - val_acc: 0.8744\n",
      "Epoch 7/8\n",
      "25000/25000 [==============================] - 85s 3ms/step - loss: 0.0455 - acc: 0.9835 - val_loss: 0.4175 - val_acc: 0.8795\n",
      "Epoch 8/8\n",
      "25000/25000 [==============================] - 86s 3ms/step - loss: 0.0398 - acc: 0.9854 - val_loss: 0.4689 - val_acc: 0.8836\n",
      "Training: accuracy = 0.998800  ;  loss = 0.006388\n",
      "Validation: accuracy1 = 0.883600  ;  loss1 = 0.468917\n"
     ]
    }
   ],
   "source": [
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 8\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'],\n",
    "             )\n",
    "model.summary()\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Validation: accuracy1 = %f  ;  loss1 = %f\" % (accuracy, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cnn_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n",
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 96, 64)            41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 24, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 70)                37800     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 71        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 2,638,895\n",
      "Trainable params: 2,638,895\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 62s 2ms/step - loss: 0.3866 - acc: 0.8199 - val_loss: 0.3418 - val_acc: 0.8480\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 61s 2ms/step - loss: 0.1982 - acc: 0.9250 - val_loss: 0.3437 - val_acc: 0.8575\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 60s 2ms/step - loss: 0.0948 - acc: 0.9672 - val_loss: 0.4149 - val_acc: 0.8420\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 61s 2ms/step - loss: 0.0441 - acc: 0.9858 - val_loss: 0.5634 - val_acc: 0.8386\n",
      "Training: accuracy = 0.996320  ;  loss = 0.014933\n",
      "Validation: accuracy1 = 0.838640  ;  loss1 = 0.563363\n"
     ]
    }
   ],
   "source": [
    "# Embedding\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "epochs = 4\n",
    "\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Validation: accuracy1 = %f  ;  loss1 = %f\" % (accuracy, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN, CNN+LSTM Model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### CNN  \n",
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_1 (Embedding)      (None, 400, 50)           250000    \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 400, 50)           0         \n",
    "_________________________________________________________________\n",
    "conv1d_1 (Conv1D)            (None, 398, 250)          37750     \n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 250)               62750     \n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 250)               0         \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 250)               0         \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 1)                 251       \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 1)                 0         \n",
    "=================================================================\n",
    "Epoch 1/8\n",
    "25000/25000 [==============================] - 87s 3ms/step - loss: 0.4027 - acc: 0.8010 - val_loss: 0.3173 - val_acc: 0.8617\n",
    "Epoch 2/8\n",
    "25000/25000 [==============================] - 86s 3ms/step - loss: 0.2280 - acc: 0.9099 - val_loss: 0.2778 - val_acc: 0.8835\n",
    "Epoch 3/8\n",
    "25000/25000 [==============================] - 86s 3ms/step - loss: 0.1627 - acc: 0.9374 - val_loss: 0.2691 - val_acc: 0.8910\n",
    "Epoch 4/8\n",
    "25000/25000 [==============================] - 86s 3ms/step - loss: 0.1114 - acc: 0.9590 - val_loss: 0.3591 - val_acc: 0.8806\n",
    "Epoch 5/8\n",
    "25000/25000 [==============================] - 86s 3ms/step - loss: 0.0791 - acc: 0.9719 - val_loss: 0.3894 - val_acc: 0.8819\n",
    "Epoch 6/8\n",
    "25000/25000 [==============================] - 86s 3ms/step - loss: 0.0575 - acc: 0.9790 - val_loss: 0.4305 - val_acc: 0.8744\n",
    "Epoch 7/8\n",
    "25000/25000 [==============================] - 85s 3ms/step - loss: 0.0455 - acc: 0.9835 - val_loss: 0.4175 - val_acc: 0.8795\n",
    "Epoch 8/8\n",
    "25000/25000 [==============================] - 86s 3ms/step - loss: 0.0398 - acc: 0.9854 - val_loss: 0.4689 - val_acc: 0.8836\n",
    "```  \n",
    "### CNN LSTM  \n",
    "```\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_3 (Embedding)      (None, 100, 128)          2560000   \n",
    "_________________________________________________________________\n",
    "dropout_4 (Dropout)          (None, 100, 128)          0         \n",
    "_________________________________________________________________\n",
    "conv1d_3 (Conv1D)            (None, 96, 64)            41024     \n",
    "_________________________________________________________________\n",
    "max_pooling1d_2 (MaxPooling1 (None, 24, 64)            0         \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 70)                37800     \n",
    "_________________________________________________________________\n",
    "dense_4 (Dense)              (None, 1)                 71        \n",
    "_________________________________________________________________\n",
    "activation_4 (Activation)    (None, 1)                 0         \n",
    "=================================================================\n",
    "Epoch 1/4\n",
    "25000/25000 [==============================] - 62s 2ms/step - loss: 0.3866 - acc: 0.8199 - val_loss: 0.3418 - val_acc: 0.8480\n",
    "Epoch 2/4\n",
    "25000/25000 [==============================] - 61s 2ms/step - loss: 0.1982 - acc: 0.9250 - val_loss: 0.3437 - val_acc: 0.8575\n",
    "Epoch 3/4\n",
    "25000/25000 [==============================] - 60s 2ms/step - loss: 0.0948 - acc: 0.9672 - val_loss: 0.4149 - val_acc: 0.8420\n",
    "Epoch 4/4\n",
    "25000/25000 [==============================] - 61s 2ms/step - loss: 0.0441 - acc: 0.9858 - val_loss: 0.5634 - val_acc: 0.8386\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "#### LSTM\n",
    "- Training: accuracy = 0.9789  ;  loss = 0.0610  \n",
    "- Validation: accuracy1 = 0.82688  ;  loss1 = 0.5830579057455063\n",
    "\n",
    "#### CNN  \n",
    "- Training: accuracy = 0.998800  ;  loss = 0.006388  \n",
    "- Validation: accuracy1 = 0.883600  ;  loss1 = 0.468917 \n",
    "\n",
    "#### CNN+LSTM  \n",
    "- Training: accuracy = 0.996320  ;  loss = 0.014933  \n",
    "- Validation: accuracy1 = 0.838640  ;  loss1 = 0.563363\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
